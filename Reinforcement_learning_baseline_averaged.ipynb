{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement_learning_baseline_averaged.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "av4t4b5g45el",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import torch  \n",
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import least_squares"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTF0WguK-F70",
        "colab_type": "code",
        "outputId": "b9004432-d4b1-4932-a172-83037c6861a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFP-x3MA5LEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Constants\n",
        "GAMMA = 0.99\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, num_inputs, num_actions, hidden_size, learning_rate=3e-4):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "\n",
        "        self.num_actions = num_actions\n",
        "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, num_actions)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.linear1(state))\n",
        "        x = F.softmax(self.linear2(x), dim=1)\n",
        "        return x \n",
        "    \n",
        "    def get_action(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "        probs = self.forward(Variable(state))\n",
        "        #Choose action with regard to policy\n",
        "        highest_prob_action = np.random.choice(self.num_actions, p=np.squeeze(probs.detach().numpy()))\n",
        "        log_prob = torch.log(probs.squeeze(0)[highest_prob_action]) #log for gradient\n",
        "        return highest_prob_action, log_prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK5vloYO5Ox5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, num_inputs, num_actions, hidden_size, learning_rate=3e-4):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "\n",
        "        self.num_actions = num_actions\n",
        "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, 1)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "    \n",
        "    def forward(self, state):\n",
        "        state = Variable(torch.from_numpy(state).float().unsqueeze(0))\n",
        "        value = F.relu(self.linear1(state))\n",
        "        value = self.linear2(value)\n",
        "        return value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fH6RMdJm5Rao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_policy(policy_network, trajectories_gradient):\n",
        "    policy_network.optimizer.zero_grad()    \n",
        "    policy_gradient = torch.stack(trajectories_gradient).sum()\n",
        "    policy_gradient.backward()\n",
        "    policy_network.optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K9WsvOe5UF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_trajectory(rewards, log_probs, baseline):\n",
        "    discounted_rewards = []\n",
        "\n",
        "    for t in range(len(rewards)):\n",
        "        Gt = 0 \n",
        "        pw = 0\n",
        "        for r in rewards[t:]:\n",
        "            Gt = Gt + GAMMA**pw * r\n",
        "            pw = pw + 1\n",
        "        discounted_rewards.append(Gt)\n",
        "        \n",
        "    discounted_rewards = torch.tensor(discounted_rewards)\n",
        "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9) # normalize discounted rewards\n",
        "\n",
        "    policy_gradient = []\n",
        "    for log_prob, Gt, bs in zip(log_probs, discounted_rewards, baseline):\n",
        "        if log_prob == 0:\n",
        "            policy_gradient.append(torch.tensor([[0.0]], requires_grad = True))\n",
        "        else:\n",
        "            policy_gradient.append(-log_prob * (Gt - bs))\n",
        "    \n",
        "    policy_gradient = torch.stack(policy_gradient).sum()\n",
        "    return policy_gradient"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4LCCInY5Xvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def align_columns_zeros(array):\n",
        "    max_length = max(list(map(lambda x: len(x), array)))\n",
        "    for col in range(len(array)):\n",
        "        array[col] = np.pad(array[col], max_length - len(array[col]), 'constant', constant_values = 0)[max_length - len(array[col]):]\n",
        "    return array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLm4i1Vh5b2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def baseline_approximation(policy_net, value_net, n_trajectories, n_epoch = 4000, early_stopping_rounds = 250):\n",
        "    max_steps = 10000\n",
        "    min_loss = float('inf')\n",
        "    stopping_rounds = 0\n",
        "    epoch = 1\n",
        "    while(stopping_rounds < early_stopping_rounds and epoch < n_epoch):\n",
        "        r_gradient = []\n",
        "        rewards = [[] for i in range(n_trajectories)]\n",
        "        log_probs = [[] for i in range(n_trajectories)]\n",
        "        baseline_values = [[] for i in range(n_trajectories)]\n",
        "    \n",
        "        for trajectory in range(n_trajectories):\n",
        "            state = env.reset()\n",
        "            \n",
        "            for steps in range(max_steps):\n",
        "                baseline = value_net.forward(state)\n",
        "                action, log_prob = policy_net.get_action(state)\n",
        "                new_state, reward, done, _ = env.step(np.array(action))\n",
        "                baseline_values[trajectory].append(baseline)\n",
        "                log_probs[trajectory].append(log_prob)\n",
        "                rewards[trajectory].append(reward)\n",
        "                if done:                    \n",
        "                    break\n",
        "                state = new_state\n",
        "        rewards = align_columns_zeros(rewards)\n",
        "        log_probs = align_columns_zeros(log_probs)\n",
        "        baseline_values = align_columns_zeros(baseline_values)\n",
        "        for col in range(len(rewards)):\n",
        "            traj = count_trajectory(rewards[col], log_probs[col], baseline_values[col])\n",
        "            traj.backward()\n",
        "            deriv = []\n",
        "            for param in policy_net.parameters():\n",
        "              deriv.append(param.grad.view(param.grad.numel()))\n",
        "            deriv = torch.cat([tensor for tensor in deriv], 0)\n",
        "            r_gradient.append(deriv)\n",
        "\n",
        "        r_gradient = torch.stack(r_gradient)\n",
        "        value_loss = torch.var(r_gradient, 1).sum()\n",
        "        if value_loss < min_loss:\n",
        "            min_loss = value_loss\n",
        "            torch.save(value_net, 'model.pth')\n",
        "            print('{}. Current minimum value loss = {}'.format(epoch, value_loss))\n",
        "            stopping_rounds = 0\n",
        "        else:\n",
        "            stopping_rounds += 1\n",
        "        epoch += 1\n",
        "        value_net.optimizer.zero_grad()\n",
        "        value_loss.backward()\n",
        "        value_net.optimizer.step()\n",
        "    value_net = torch.load('model.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuJiO0Tm5ifD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cart_pole_baseline(value_net, n_trajectories = 2, episode_num = 1500, baseline_retrain = False, retrain_episodes = 50):\n",
        "    env = gym.make('CartPole-v0')\n",
        "    policy_net = PolicyNetwork(env.observation_space.shape[0], env.action_space.n, 256)\n",
        "    \n",
        "\n",
        "    max_episode_num = episode_num\n",
        "    max_steps = 10000\n",
        "    numsteps = [[] for i in range(n_trajectories)]\n",
        "    avg_numsteps = [[] for i in range(n_trajectories)]\n",
        "    all_rewards = [[] for i in range(n_trajectories)]\n",
        "\n",
        "    for episode in range(1, max_episode_num + 1):\n",
        "        r_gradient = []\n",
        "        rewards = [[] for i in range(n_trajectories)]\n",
        "        baseline_values = []\n",
        "        \n",
        "        for trajectory in range(n_trajectories):\n",
        "            state = env.reset()\n",
        "            log_probs = []\n",
        "            \n",
        "            for steps in range(max_steps):\n",
        "                #env.render()\n",
        "                baseline = value_net.forward(state)\n",
        "                action, log_prob = policy_net.get_action(state)\n",
        "                new_state, reward, done, _ = env.step(action)\n",
        "                log_probs.append(log_prob)\n",
        "                rewards[trajectory].append(reward)\n",
        "                baseline_values.append(baseline)\n",
        "                \n",
        "                if done:\n",
        "                    traj = count_trajectory(rewards[trajectory], log_probs, baseline_values)                       \n",
        "                    r_gradient.append(traj)\n",
        "                    numsteps[trajectory].append(steps)\n",
        "                    avg_numsteps[trajectory].append(np.mean(numsteps[trajectory][-10:]))\n",
        "                    all_rewards[trajectory].append(np.sum(rewards[trajectory]))\n",
        "                    break\n",
        "\n",
        "                state = new_state\n",
        "                \n",
        "        update_policy(policy_net, r_gradient)                 \n",
        "        rewards = align_columns_zeros(rewards)\n",
        "        #print(all_rewards)\n",
        "        if episode % 10 == 0:\n",
        "            sys.stdout.write(\"episode: {}, total mean reward among trajectories: {}, average_reward_among_trajectories: {}\\n\".\\\n",
        "                                     format(episode, np.round(np.mean(np.sum(rewards, axis = 1)), decimals = 3),\\\n",
        "                                            np.round(np.mean(np.mean(all_rewards, axis = 0)[-10:]), decimals = 3)))\n",
        "            \n",
        "        if baseline_retrain == True:\n",
        "            if episode % retrain_episodes == 0:\n",
        "                baseline_approximation(policy_net, value_net, 3, n_epoch = 1000, early_stopping_rounds=10)\n",
        "        \n",
        "    return all_rewards, avg_numsteps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-IAQduN5n25",
        "colab_type": "code",
        "outputId": "21fc86e3-ad7e-4cd5-bef6-77166f8178f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "env = gym.make('CartPole-v0')\n",
        "policy_net = PolicyNetwork(env.observation_space.shape[0], env.action_space.n, 256)\n",
        "value_net = ValueNetwork(env.observation_space.shape[0], env.action_space.n, 256)\n",
        "baseline_approximation(policy_net, value_net, 15, early_stopping_rounds=300)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. Current minimum value loss = 7.86085319519043\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type ValueNetwork. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-161-d45828a1751a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"env = gym.make('CartPole-v0')\\npolicy_net = PolicyNetwork(env.observation_space.shape[0], env.action_space.n, 256)\\nvalue_net = ValueNetwork(env.observation_space.shape[0], env.action_space.n, 256)\\nbaseline_approximation(policy_net, value_net, 15, early_stopping_rounds=300)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-160-504b0a35076e>\u001b[0m in \u001b[0;36mbaseline_approximation\u001b[0;34m(policy_net, value_net, n_trajectories, n_epoch, early_stopping_rounds)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mvalue_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mvalue_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mvalue_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mvalue_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Fx1DoY25rcI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "statistics_all = []\n",
        "statistics_mean = []\n",
        "for game in tqdm(range(10)):\n",
        "  all_rewards, mean_rewards = cart_pole(1, 2000)\n",
        "  statistics_all.append(all_rewards)\n",
        "  statistics_mean.append(mean_rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nw_4rhGv5sxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}