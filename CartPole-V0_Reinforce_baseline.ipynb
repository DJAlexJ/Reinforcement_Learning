{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch  \n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE BASELINE VALUE FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "GAMMA = 0.99\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, learning_rate=3e-4):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_actions)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.softmax(self.linear2(x), dim=1)\n",
    "        return x \n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        probs = self.forward(Variable(state))\n",
    "        #Choose action with regard to policy\n",
    "        highest_prob_action = np.random.choice(self.num_actions, p=np.squeeze(probs.detach().numpy()))\n",
    "        log_prob = torch.log(probs.squeeze(0)[highest_prob_action]) #log for gradient\n",
    "        return highest_prob_action, log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, learning_rate=3e-4):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        state = Variable(torch.from_numpy(state).float().unsqueeze(0))\n",
    "        value = F.relu(self.linear1(state))\n",
    "        value = self.linear2(value)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(policy_network, trajectories_gradient):\n",
    "    policy_network.optimizer.zero_grad()    \n",
    "    policy_gradient = torch.stack(trajectories_gradient).sum()\n",
    "    policy_gradient.backward()\n",
    "    policy_network.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trajectory(rewards, log_probs, baseline, policy_learning = False):\n",
    "    discounted_rewards = []\n",
    "\n",
    "    for t in range(len(rewards)):\n",
    "        Gt = 0 \n",
    "        pw = 0\n",
    "        for r in rewards[t:]:\n",
    "            Gt = Gt + GAMMA**pw * r\n",
    "            pw = pw + 1\n",
    "        discounted_rewards.append(Gt)\n",
    "        \n",
    "    discounted_rewards = torch.tensor(discounted_rewards)\n",
    "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9) # normalize discounted rewards\n",
    "\n",
    "    policy_gradient = []\n",
    "    for log_prob, Gt, bs in zip(log_probs, discounted_rewards, baseline):\n",
    "        if log_prob == 0:\n",
    "            policy_gradient.append(torch.tensor([[0.0]], requires_grad = True))\n",
    "        else:\n",
    "            policy_gradient.append(-log_prob * (Gt - bs))\n",
    "    \n",
    "    if policy_learning:\n",
    "        policy_gradient = torch.stack(policy_gradient).sum()\n",
    "    return policy_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_columns_zeros(array):\n",
    "    max_length = max(list(map(lambda x: len(x), array)))\n",
    "    for col in range(len(array)):\n",
    "        array[col] = np.pad(array[col], max_length - len(array[col]), 'constant', constant_values = 0)[max_length - len(array[col]):]\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_approximation(policy_net, value_net, val_optimizer, n_trajectories, n_epoch = 4000, early_stopping_rounds = 250):\n",
    "    max_steps = 10000\n",
    "    min_loss = float('inf')\n",
    "    stopping_rounds = 0\n",
    "    epoch = 1\n",
    "    while(stopping_rounds < early_stopping_rounds and epoch < n_epoch):\n",
    "        r_gradient = []\n",
    "        rewards = [[] for i in range(n_trajectories)]\n",
    "        log_probs = [[] for i in range(n_trajectories)]\n",
    "        baseline_values = [[] for i in range(n_trajectories)]\n",
    "    \n",
    "        for trajectory in range(n_trajectories):\n",
    "            state = env.reset()\n",
    "            \n",
    "            for steps in range(max_steps):\n",
    "                baseline = value_net.forward(state)\n",
    "                action, log_prob = policy_net.get_action(state)\n",
    "                new_state, reward, done, _ = env.step(action)\n",
    "                baseline_values[trajectory].append(baseline)\n",
    "                log_probs[trajectory].append(log_prob)\n",
    "                rewards[trajectory].append(reward)\n",
    "                if done:                    \n",
    "                    break\n",
    "                state = new_state\n",
    "        rewards = align_columns_zeros(rewards)\n",
    "        log_probs = align_columns_zeros(log_probs)\n",
    "        baseline_values = align_columns_zeros(baseline_values)\n",
    "        for col in range(len(rewards)):\n",
    "            traj = count_trajectory(rewards[col], log_probs[col], baseline_values[col])\n",
    "            r_gradient.append(traj)\n",
    "            \n",
    "        r_gradient = list(map(lambda x: torch.stack(x), r_gradient))\n",
    "        r_gradient = torch.stack(r_gradient)\n",
    "        \n",
    "        value_loss = torch.var(r_gradient, 1).sum()\n",
    "        if value_loss < min_loss:\n",
    "            min_loss = value_loss\n",
    "            torch.save(value_net, 'model.pth')\n",
    "            print('{}. Current minimum value loss = {}'.format(epoch, value_loss))\n",
    "            stopping_rounds = 0\n",
    "        else:\n",
    "            stopping_rounds += 1\n",
    "        #print('{}. Value loss = {}, early stopping rounds = {}'.format(epoch, value_loss, stopping_rounds))\n",
    "        epoch += 1\n",
    "        val_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        val_optimizer.step()\n",
    "    value_net = torch.load('model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cart_pole_baseline(value_net, n_trajectories = 2, episode_num = 1500, baseline_retrain = False, retrain_episodes = 50):\n",
    "    env = gym.make('CartPole-v0')\n",
    "    policy_net = PolicyNetwork(env.observation_space.shape[0], env.action_space.n, 256)\n",
    "    \n",
    "\n",
    "    max_episode_num = episode_num\n",
    "    max_steps = 10000\n",
    "    numsteps = [[] for i in range(n_trajectories)]\n",
    "    avg_numsteps = [[] for i in range(n_trajectories)]\n",
    "    all_rewards = [[] for i in range(n_trajectories)]\n",
    "\n",
    "    for episode in range(1, max_episode_num + 1):\n",
    "        r_gradient = []\n",
    "        rewards = [[] for i in range(n_trajectories)]\n",
    "        baseline_values = []\n",
    "        \n",
    "        for trajectory in range(n_trajectories):\n",
    "            state = env.reset()\n",
    "            log_probs = []\n",
    "            \n",
    "            for steps in range(max_steps):\n",
    "                #env.render()\n",
    "                baseline = value_net.forward(state)\n",
    "                action, log_prob = policy_net.get_action(state)\n",
    "                new_state, reward, done, _ = env.step(action)\n",
    "                log_probs.append(log_prob)\n",
    "                rewards[trajectory].append(reward)\n",
    "                baseline_values.append(baseline)\n",
    "                \n",
    "                if done:\n",
    "                    traj = count_trajectory(rewards[trajectory], log_probs, baseline_values, True)                       \n",
    "                    r_gradient.append(traj)\n",
    "                    numsteps[trajectory].append(steps)\n",
    "                    avg_numsteps[trajectory].append(np.mean(numsteps[trajectory][-10:]))\n",
    "                    all_rewards[trajectory].append(np.sum(rewards[trajectory]))\n",
    "                    break\n",
    "\n",
    "                state = new_state\n",
    "                \n",
    "        update_policy(policy_net, r_gradient)                 \n",
    "        rewards = align_columns_zeros(rewards)\n",
    "        #print(all_rewards)\n",
    "        if episode % 10 == 0:\n",
    "            sys.stdout.write(\"episode: {}, total mean reward among trajectories: {}, average_reward_among_trajectories: {}\\n\".\\\n",
    "                                     format(episode, np.round(np.mean(np.sum(rewards, axis = 1)), decimals = 3),\\\n",
    "                                            np.round(np.mean(np.mean(all_rewards, axis = 0)[-10:]), decimals = 3)))\n",
    "            \n",
    "        if baseline_retrain == True:\n",
    "            if episode % retrain_episodes == 0:\n",
    "                baseline_approximation(policy_net, value_net, val_optimizer, 3, n_epoch = 1000, early_stopping_rounds=10)\n",
    "        \n",
    "    return all_rewards, avg_numsteps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ValueNetwork. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Current minimum value loss = 1.0817387104034424\n",
      "2. Current minimum value loss = 1.0429147481918335\n",
      "5. Current minimum value loss = 0.9826594591140747\n",
      "7. Current minimum value loss = 0.940153956413269\n",
      "13. Current minimum value loss = 0.9185212254524231\n",
      "14. Current minimum value loss = 0.8809243440628052\n",
      "16. Current minimum value loss = 0.8070756793022156\n",
      "28. Current minimum value loss = 0.7866318225860596\n",
      "54. Current minimum value loss = 0.7299036383628845\n",
      "59. Current minimum value loss = 0.6481955051422119\n",
      "65. Current minimum value loss = 0.6434944868087769\n",
      "98. Current minimum value loss = 0.5985133647918701\n",
      "128. Current minimum value loss = 0.5692092180252075\n",
      "170. Current minimum value loss = 0.5492762327194214\n",
      "171. Current minimum value loss = 0.42624300718307495\n",
      "197. Current minimum value loss = 0.3594302535057068\n",
      "216. Current minimum value loss = 0.35601943731307983\n",
      "246. Current minimum value loss = 0.3036741018295288\n",
      "289. Current minimum value loss = 0.27783671021461487\n",
      "291. Current minimum value loss = 0.27698707580566406\n",
      "351. Current minimum value loss = 0.24368658661842346\n",
      "430. Current minimum value loss = 0.15131141245365143\n",
      "593. Current minimum value loss = 0.12932631373405457\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "env = gym.make('CartPole-v0')\n",
    "policy_net = PolicyNetwork(env.observation_space.shape[0], env.action_space.n, 256)\n",
    "value_net = ValueNetwork(env.observation_space.shape[0], env.action_space.n, 256)\n",
    "val_optimizer = optim.Adam(value_net.parameters(), lr=3e-4)\n",
    "baseline_approximation(policy_net, value_net, val_optimizer, 3, early_stopping_rounds=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10, total mean reward among trajectories: 25.0, average_reward_among_trajectories: 25.45\n",
      "1. Current minimum value loss = 0.4894123673439026\n",
      "4. Current minimum value loss = 0.3327412009239197\n",
      "8. Current minimum value loss = 0.2895969748497009\n",
      "16. Current minimum value loss = 0.2633309066295624\n",
      "episode: 20, total mean reward among trajectories: 17.0, average_reward_among_trajectories: 29.9\n",
      "1. Current minimum value loss = 0.19558849930763245\n",
      "episode: 30, total mean reward among trajectories: 23.0, average_reward_among_trajectories: 21.85\n",
      "1. Current minimum value loss = 0.4791482985019684\n",
      "10. Current minimum value loss = 0.36857396364212036\n",
      "11. Current minimum value loss = 0.2799012064933777\n",
      "episode: 40, total mean reward among trajectories: 18.0, average_reward_among_trajectories: 21.6\n",
      "1. Current minimum value loss = 0.5185263156890869\n",
      "2. Current minimum value loss = 0.3047350347042084\n",
      "episode: 50, total mean reward among trajectories: 60.0, average_reward_among_trajectories: 29.7\n",
      "1. Current minimum value loss = 0.46863266825675964\n",
      "2. Current minimum value loss = 0.4613702595233917\n",
      "3. Current minimum value loss = 0.3015313446521759\n",
      "episode: 60, total mean reward among trajectories: 15.0, average_reward_among_trajectories: 28.0\n",
      "1. Current minimum value loss = 0.3760966658592224\n",
      "episode: 70, total mean reward among trajectories: 60.0, average_reward_among_trajectories: 31.8\n",
      "1. Current minimum value loss = 0.7589477300643921\n",
      "2. Current minimum value loss = 0.664898157119751\n",
      "3. Current minimum value loss = 0.5837448835372925\n",
      "8. Current minimum value loss = 0.4608212411403656\n",
      "17. Current minimum value loss = 0.3887780010700226\n",
      "episode: 80, total mean reward among trajectories: 54.5, average_reward_among_trajectories: 33.15\n",
      "1. Current minimum value loss = 0.8532864451408386\n",
      "2. Current minimum value loss = 0.6380295753479004\n",
      "3. Current minimum value loss = 0.5267211198806763\n",
      "5. Current minimum value loss = 0.4223313331604004\n",
      "8. Current minimum value loss = 0.27321574091911316\n",
      "episode: 90, total mean reward among trajectories: 24.0, average_reward_among_trajectories: 30.2\n",
      "1. Current minimum value loss = 0.900663435459137\n",
      "2. Current minimum value loss = 0.6745126247406006\n",
      "3. Current minimum value loss = 0.5626593828201294\n",
      "6. Current minimum value loss = 0.324214369058609\n",
      "episode: 100, total mean reward among trajectories: 24.5, average_reward_among_trajectories: 28.35\n",
      "1. Current minimum value loss = 0.45451268553733826\n",
      "episode: 110, total mean reward among trajectories: 35.5, average_reward_among_trajectories: 32.55\n",
      "1. Current minimum value loss = 0.7900040745735168\n",
      "3. Current minimum value loss = 0.7132450342178345\n",
      "4. Current minimum value loss = 0.6203716993331909\n",
      "7. Current minimum value loss = 0.5215461254119873\n",
      "8. Current minimum value loss = 0.48097947239875793\n",
      "13. Current minimum value loss = 0.4446721076965332\n",
      "20. Current minimum value loss = 0.4013986587524414\n",
      "24. Current minimum value loss = 0.37704864144325256\n",
      "32. Current minimum value loss = 0.24496129155158997\n",
      "episode: 120, total mean reward among trajectories: 21.5, average_reward_among_trajectories: 30.2\n",
      "1. Current minimum value loss = 0.789257824420929\n",
      "3. Current minimum value loss = 0.48666971921920776\n",
      "8. Current minimum value loss = 0.47498032450675964\n",
      "15. Current minimum value loss = 0.4326710104942322\n",
      "episode: 130, total mean reward among trajectories: 23.5, average_reward_among_trajectories: 27.8\n",
      "1. Current minimum value loss = 0.647741436958313\n",
      "2. Current minimum value loss = 0.3022783100605011\n",
      "episode: 140, total mean reward among trajectories: 47.0, average_reward_among_trajectories: 38.2\n",
      "1. Current minimum value loss = 0.6269052028656006\n",
      "3. Current minimum value loss = 0.49613267183303833\n",
      "10. Current minimum value loss = 0.3677946925163269\n",
      "episode: 150, total mean reward among trajectories: 36.5, average_reward_among_trajectories: 40.75\n",
      "1. Current minimum value loss = 0.7227433323860168\n",
      "2. Current minimum value loss = 0.4859042465686798\n",
      "episode: 160, total mean reward among trajectories: 53.5, average_reward_among_trajectories: 43.05\n",
      "1. Current minimum value loss = 0.5015636086463928\n",
      "3. Current minimum value loss = 0.44694873690605164\n",
      "5. Current minimum value loss = 0.38157331943511963\n",
      "episode: 170, total mean reward among trajectories: 49.5, average_reward_among_trajectories: 50.45\n",
      "1. Current minimum value loss = 0.60286545753479\n",
      "2. Current minimum value loss = 0.5497099161148071\n",
      "3. Current minimum value loss = 0.43823856115341187\n",
      "9. Current minimum value loss = 0.43174809217453003\n",
      "13. Current minimum value loss = 0.30159300565719604\n",
      "episode: 180, total mean reward among trajectories: 38.0, average_reward_among_trajectories: 35.85\n",
      "1. Current minimum value loss = 0.9290857315063477\n",
      "2. Current minimum value loss = 0.5440548658370972\n",
      "4. Current minimum value loss = 0.4206428527832031\n",
      "11. Current minimum value loss = 0.4203096330165863\n",
      "16. Current minimum value loss = 0.31633108854293823\n",
      "episode: 190, total mean reward among trajectories: 25.5, average_reward_among_trajectories: 46.3\n",
      "1. Current minimum value loss = 0.5913684964179993\n",
      "2. Current minimum value loss = 0.3842100501060486\n",
      "9. Current minimum value loss = 0.3379209041595459\n",
      "episode: 200, total mean reward among trajectories: 31.0, average_reward_among_trajectories: 41.9\n",
      "1. Current minimum value loss = 0.7788101434707642\n",
      "2. Current minimum value loss = 0.5220276713371277\n",
      "3. Current minimum value loss = 0.4468596577644348\n",
      "10. Current minimum value loss = 0.36674565076828003\n",
      "15. Current minimum value loss = 0.3159099221229553\n",
      "episode: 210, total mean reward among trajectories: 39.0, average_reward_among_trajectories: 51.05\n",
      "1. Current minimum value loss = 0.4170837998390198\n",
      "9. Current minimum value loss = 0.27300888299942017\n",
      "episode: 220, total mean reward among trajectories: 51.5, average_reward_among_trajectories: 49.5\n",
      "1. Current minimum value loss = 0.7027193903923035\n",
      "2. Current minimum value loss = 0.5037878751754761\n",
      "4. Current minimum value loss = 0.30011817812919617\n",
      "5. Current minimum value loss = 0.26006966829299927\n",
      "episode: 230, total mean reward among trajectories: 58.5, average_reward_among_trajectories: 52.35\n",
      "1. Current minimum value loss = 0.54658043384552\n",
      "2. Current minimum value loss = 0.2140074521303177\n",
      "episode: 240, total mean reward among trajectories: 62.0, average_reward_among_trajectories: 72.4\n",
      "1. Current minimum value loss = 0.6777169704437256\n",
      "3. Current minimum value loss = 0.6245895624160767\n",
      "5. Current minimum value loss = 0.3041119873523712\n",
      "episode: 250, total mean reward among trajectories: 111.5, average_reward_among_trajectories: 69.7\n",
      "1. Current minimum value loss = 0.5282392501831055\n",
      "4. Current minimum value loss = 0.3928002119064331\n",
      "7. Current minimum value loss = 0.24205143749713898\n",
      "8. Current minimum value loss = 0.1867918074131012\n",
      "18. Current minimum value loss = 0.1850380152463913\n",
      "22. Current minimum value loss = 0.14826667308807373\n",
      "episode: 260, total mean reward among trajectories: 67.0, average_reward_among_trajectories: 76.4\n",
      "1. Current minimum value loss = 0.2443844974040985\n",
      "6. Current minimum value loss = 0.0642409399151802\n",
      "episode: 270, total mean reward among trajectories: 106.5, average_reward_among_trajectories: 66.25\n",
      "1. Current minimum value loss = 0.27858054637908936\n",
      "10. Current minimum value loss = 0.258247047662735\n",
      "14. Current minimum value loss = 0.24323377013206482\n",
      "18. Current minimum value loss = 0.1761428266763687\n",
      "episode: 280, total mean reward among trajectories: 44.5, average_reward_among_trajectories: 60.15\n",
      "1. Current minimum value loss = 0.3350570797920227\n",
      "3. Current minimum value loss = 0.256578654050827\n",
      "5. Current minimum value loss = 0.2565230429172516\n",
      "14. Current minimum value loss = 0.12210887670516968\n",
      "episode: 290, total mean reward among trajectories: 87.5, average_reward_among_trajectories: 70.2\n",
      "1. Current minimum value loss = 0.3263024389743805\n",
      "5. Current minimum value loss = 0.29334142804145813\n",
      "7. Current minimum value loss = 0.2243582010269165\n",
      "14. Current minimum value loss = 0.15044502913951874\n",
      "episode: 300, total mean reward among trajectories: 124.0, average_reward_among_trajectories: 75.65\n",
      "1. Current minimum value loss = 0.7225796580314636\n",
      "2. Current minimum value loss = 0.5508248805999756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Current minimum value loss = 0.2950555682182312\n",
      "7. Current minimum value loss = 0.20717623829841614\n",
      "episode: 310, total mean reward among trajectories: 110.5, average_reward_among_trajectories: 95.4\n",
      "1. Current minimum value loss = 0.47972458600997925\n",
      "2. Current minimum value loss = 0.3588549494743347\n",
      "3. Current minimum value loss = 0.28334301710128784\n",
      "4. Current minimum value loss = 0.21110138297080994\n",
      "episode: 320, total mean reward among trajectories: 129.5, average_reward_among_trajectories: 99.65\n",
      "1. Current minimum value loss = 0.16397108137607574\n",
      "episode: 330, total mean reward among trajectories: 130.0, average_reward_among_trajectories: 102.05\n",
      "1. Current minimum value loss = 0.1288338154554367\n",
      "7. Current minimum value loss = 0.1154736578464508\n",
      "episode: 340, total mean reward among trajectories: 79.5, average_reward_among_trajectories: 97.75\n",
      "1. Current minimum value loss = 0.4433952271938324\n",
      "2. Current minimum value loss = 0.211518332362175\n",
      "8. Current minimum value loss = 0.16510799527168274\n",
      "episode: 350, total mean reward among trajectories: 54.5, average_reward_among_trajectories: 92.8\n",
      "1. Current minimum value loss = 0.4553315043449402\n",
      "3. Current minimum value loss = 0.11214570701122284\n",
      "11. Current minimum value loss = 0.07188340276479721\n",
      "episode: 360, total mean reward among trajectories: 142.5, average_reward_among_trajectories: 117.15\n",
      "1. Current minimum value loss = 0.17089694738388062\n",
      "2. Current minimum value loss = 0.15494123101234436\n",
      "7. Current minimum value loss = 0.13247308135032654\n",
      "episode: 370, total mean reward among trajectories: 131.5, average_reward_among_trajectories: 115.65\n",
      "1. Current minimum value loss = 0.6523669362068176\n",
      "3. Current minimum value loss = 0.4973089396953583\n",
      "4. Current minimum value loss = 0.138550266623497\n",
      "12. Current minimum value loss = 0.08676603436470032\n",
      "episode: 380, total mean reward among trajectories: 51.0, average_reward_among_trajectories: 101.95\n",
      "1. Current minimum value loss = 0.44109949469566345\n",
      "2. Current minimum value loss = 0.16834479570388794\n",
      "10. Current minimum value loss = 0.14437083899974823\n",
      "episode: 390, total mean reward among trajectories: 151.0, average_reward_among_trajectories: 118.25\n",
      "1. Current minimum value loss = 0.18678231537342072\n",
      "episode: 400, total mean reward among trajectories: 133.5, average_reward_among_trajectories: 133.3\n",
      "1. Current minimum value loss = 0.3829825222492218\n",
      "4. Current minimum value loss = 0.1972956359386444\n",
      "episode: 410, total mean reward among trajectories: 116.0, average_reward_among_trajectories: 138.15\n",
      "1. Current minimum value loss = 0.2945472002029419\n",
      "2. Current minimum value loss = 0.2721644639968872\n",
      "10. Current minimum value loss = 0.2085224986076355\n",
      "12. Current minimum value loss = 0.20243339240550995\n",
      "17. Current minimum value loss = 0.12661655247211456\n",
      "18. Current minimum value loss = 0.12334944307804108\n",
      "episode: 420, total mean reward among trajectories: 165.5, average_reward_among_trajectories: 125.7\n",
      "1. Current minimum value loss = 0.5339391827583313\n",
      "2. Current minimum value loss = 0.12646952271461487\n",
      "episode: 430, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 149.05\n",
      "1. Current minimum value loss = 0.5160396695137024\n",
      "3. Current minimum value loss = 0.34067797660827637\n",
      "4. Current minimum value loss = 0.13521969318389893\n",
      "5. Current minimum value loss = 0.1048530638217926\n",
      "episode: 440, total mean reward among trajectories: 100.0, average_reward_among_trajectories: 127.8\n",
      "1. Current minimum value loss = 0.27790623903274536\n",
      "5. Current minimum value loss = 0.224321186542511\n",
      "10. Current minimum value loss = 0.2007429003715515\n",
      "19. Current minimum value loss = 0.18833741545677185\n",
      "21. Current minimum value loss = 0.11510204523801804\n",
      "24. Current minimum value loss = 0.10401888936758041\n",
      "episode: 450, total mean reward among trajectories: 163.5, average_reward_among_trajectories: 167.8\n",
      "1. Current minimum value loss = 0.879309892654419\n",
      "2. Current minimum value loss = 0.5549573302268982\n",
      "4. Current minimum value loss = 0.5246269106864929\n",
      "8. Current minimum value loss = 0.2028336077928543\n",
      "14. Current minimum value loss = 0.19968090951442719\n",
      "23. Current minimum value loss = 0.17263899743556976\n",
      "32. Current minimum value loss = 0.15676647424697876\n",
      "episode: 460, total mean reward among trajectories: 70.0, average_reward_among_trajectories: 163.55\n",
      "1. Current minimum value loss = 0.5050835609436035\n",
      "2. Current minimum value loss = 0.24214282631874084\n",
      "6. Current minimum value loss = 0.2059817761182785\n",
      "7. Current minimum value loss = 0.17980605363845825\n",
      "episode: 470, total mean reward among trajectories: 117.0, average_reward_among_trajectories: 151.05\n",
      "1. Current minimum value loss = 0.646449863910675\n",
      "3. Current minimum value loss = 0.39760276675224304\n",
      "6. Current minimum value loss = 0.38180190324783325\n",
      "9. Current minimum value loss = 0.09729717671871185\n",
      "episode: 480, total mean reward among trajectories: 165.5, average_reward_among_trajectories: 165.9\n",
      "1. Current minimum value loss = 0.21618729829788208\n",
      "4. Current minimum value loss = 0.1873420774936676\n",
      "12. Current minimum value loss = 0.13729864358901978\n",
      "21. Current minimum value loss = 0.12592731416225433\n",
      "episode: 490, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 163.1\n",
      "1. Current minimum value loss = 1.1273393630981445\n",
      "2. Current minimum value loss = 0.23339688777923584\n",
      "10. Current minimum value loss = 0.18524357676506042\n",
      "episode: 500, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 163.15\n",
      "1. Current minimum value loss = 0.8461346626281738\n",
      "2. Current minimum value loss = 0.48532968759536743\n",
      "5. Current minimum value loss = 0.3008018136024475\n",
      "6. Current minimum value loss = 0.24217350780963898\n",
      "episode: 510, total mean reward among trajectories: 179.0, average_reward_among_trajectories: 174.2\n",
      "1. Current minimum value loss = 0.5263425707817078\n",
      "3. Current minimum value loss = 0.14193089306354523\n",
      "episode: 520, total mean reward among trajectories: 164.5, average_reward_among_trajectories: 165.0\n",
      "1. Current minimum value loss = 0.12637542188167572\n",
      "episode: 530, total mean reward among trajectories: 137.0, average_reward_among_trajectories: 158.1\n",
      "1. Current minimum value loss = 0.42790353298187256\n",
      "5. Current minimum value loss = 0.14772933721542358\n",
      "9. Current minimum value loss = 0.1114879846572876\n",
      "episode: 540, total mean reward among trajectories: 147.0, average_reward_among_trajectories: 169.05\n",
      "1. Current minimum value loss = 0.41290634870529175\n",
      "3. Current minimum value loss = 0.260814368724823\n",
      "5. Current minimum value loss = 0.2550308406352997\n",
      "11. Current minimum value loss = 0.10236142575740814\n",
      "episode: 550, total mean reward among trajectories: 126.5, average_reward_among_trajectories: 153.5\n",
      "1. Current minimum value loss = 0.8444352149963379\n",
      "2. Current minimum value loss = 0.786740779876709\n",
      "3. Current minimum value loss = 0.31821903586387634\n",
      "4. Current minimum value loss = 0.2560298442840576\n",
      "episode: 560, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 173.1\n",
      "1. Current minimum value loss = 0.6355719566345215\n",
      "2. Current minimum value loss = 0.49571463465690613\n",
      "7. Current minimum value loss = 0.2409132719039917\n",
      "10. Current minimum value loss = 0.15364094078540802\n",
      "16. Current minimum value loss = 0.10575851798057556\n",
      "episode: 570, total mean reward among trajectories: 199.5, average_reward_among_trajectories: 173.3\n",
      "1. Current minimum value loss = 0.4411497116088867\n",
      "2. Current minimum value loss = 0.34723106026649475\n",
      "4. Current minimum value loss = 0.15278851985931396\n",
      "14. Current minimum value loss = 0.1493554562330246\n",
      "episode: 580, total mean reward among trajectories: 150.0, average_reward_among_trajectories: 179.05\n",
      "1. Current minimum value loss = 0.33569854497909546\n",
      "8. Current minimum value loss = 0.2581693232059479\n",
      "11. Current minimum value loss = 0.18308186531066895\n",
      "episode: 590, total mean reward among trajectories: 172.5, average_reward_among_trajectories: 178.45\n",
      "1. Current minimum value loss = 0.4438059329986572\n",
      "2. Current minimum value loss = 0.346752792596817\n",
      "6. Current minimum value loss = 0.29964208602905273\n",
      "9. Current minimum value loss = 0.24406684935092926\n",
      "11. Current minimum value loss = 0.21893897652626038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15. Current minimum value loss = 0.16754667460918427\n",
      "episode: 600, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 170.1\n",
      "1. Current minimum value loss = 0.2010284811258316\n",
      "episode: 610, total mean reward among trajectories: 128.5, average_reward_among_trajectories: 173.25\n",
      "1. Current minimum value loss = 0.4006071090698242\n",
      "3. Current minimum value loss = 0.24876931309700012\n",
      "6. Current minimum value loss = 0.16822116076946259\n",
      "9. Current minimum value loss = 0.07580051571130753\n",
      "episode: 620, total mean reward among trajectories: 183.5, average_reward_among_trajectories: 175.1\n",
      "1. Current minimum value loss = 0.12813374400138855\n",
      "episode: 630, total mean reward among trajectories: 143.0, average_reward_among_trajectories: 174.0\n",
      "1. Current minimum value loss = 0.15412072837352753\n",
      "11. Current minimum value loss = 0.11783882975578308\n",
      "12. Current minimum value loss = 0.09531998634338379\n",
      "episode: 640, total mean reward among trajectories: 121.0, average_reward_among_trajectories: 167.35\n",
      "1. Current minimum value loss = 0.42028021812438965\n",
      "5. Current minimum value loss = 0.1594153493642807\n",
      "14. Current minimum value loss = 0.15630453824996948\n",
      "episode: 650, total mean reward among trajectories: 93.5, average_reward_among_trajectories: 167.25\n",
      "1. Current minimum value loss = 0.8580840826034546\n",
      "3. Current minimum value loss = 0.3872973918914795\n",
      "4. Current minimum value loss = 0.3687816858291626\n",
      "11. Current minimum value loss = 0.30201977491378784\n",
      "12. Current minimum value loss = 0.2673787772655487\n",
      "21. Current minimum value loss = 0.1827208548784256\n",
      "24. Current minimum value loss = 0.15500128269195557\n",
      "episode: 660, total mean reward among trajectories: 194.0, average_reward_among_trajectories: 167.4\n",
      "1. Current minimum value loss = 0.38944676518440247\n",
      "5. Current minimum value loss = 0.2874903976917267\n",
      "8. Current minimum value loss = 0.27344810962677\n",
      "12. Current minimum value loss = 0.16206279397010803\n",
      "episode: 670, total mean reward among trajectories: 198.5, average_reward_among_trajectories: 181.15\n",
      "1. Current minimum value loss = 0.3195558190345764\n",
      "11. Current minimum value loss = 0.31239235401153564\n",
      "17. Current minimum value loss = 0.14064806699752808\n",
      "episode: 680, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 181.8\n",
      "1. Current minimum value loss = 0.4839000701904297\n",
      "5. Current minimum value loss = 0.26314982771873474\n",
      "episode: 690, total mean reward among trajectories: 193.5, average_reward_among_trajectories: 172.0\n",
      "1. Current minimum value loss = 1.2023953199386597\n",
      "2. Current minimum value loss = 0.9238876700401306\n",
      "3. Current minimum value loss = 0.5551137924194336\n",
      "4. Current minimum value loss = 0.1807924062013626\n",
      "14. Current minimum value loss = 0.12368994951248169\n",
      "episode: 700, total mean reward among trajectories: 167.0, average_reward_among_trajectories: 189.95\n",
      "1. Current minimum value loss = 1.1642435789108276\n",
      "2. Current minimum value loss = 0.3388647437095642\n",
      "4. Current minimum value loss = 0.12565669417381287\n",
      "episode: 710, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 180.2\n",
      "1. Current minimum value loss = 0.15611444413661957\n",
      "episode: 720, total mean reward among trajectories: 167.5, average_reward_among_trajectories: 175.0\n",
      "1. Current minimum value loss = 0.5516895651817322\n",
      "3. Current minimum value loss = 0.2500590682029724\n",
      "episode: 730, total mean reward among trajectories: 159.5, average_reward_among_trajectories: 170.35\n",
      "1. Current minimum value loss = 0.5665144324302673\n",
      "2. Current minimum value loss = 0.5457327961921692\n",
      "6. Current minimum value loss = 0.2515474557876587\n",
      "12. Current minimum value loss = 0.18040791153907776\n",
      "episode: 740, total mean reward among trajectories: 117.5, average_reward_among_trajectories: 162.1\n",
      "1. Current minimum value loss = 1.2237409353256226\n",
      "2. Current minimum value loss = 0.5228633880615234\n",
      "4. Current minimum value loss = 0.522315263748169\n",
      "5. Current minimum value loss = 0.2108735740184784\n",
      "10. Current minimum value loss = 0.1939414143562317\n",
      "12. Current minimum value loss = 0.1413903534412384\n",
      "17. Current minimum value loss = 0.11395374685525894\n",
      "19. Current minimum value loss = 0.11142373830080032\n",
      "episode: 750, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 177.05\n",
      "1. Current minimum value loss = 0.18673132359981537\n",
      "3. Current minimum value loss = 0.1654505729675293\n",
      "episode: 760, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 167.85\n",
      "1. Current minimum value loss = 0.4277955889701843\n",
      "3. Current minimum value loss = 0.3503418564796448\n",
      "5. Current minimum value loss = 0.07829096913337708\n",
      "episode: 770, total mean reward among trajectories: 170.0, average_reward_among_trajectories: 171.0\n",
      "1. Current minimum value loss = 0.1995929777622223\n",
      "10. Current minimum value loss = 0.1833399534225464\n",
      "15. Current minimum value loss = 0.17409910261631012\n",
      "20. Current minimum value loss = 0.09583111852407455\n",
      "episode: 780, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 175.2\n",
      "1. Current minimum value loss = 1.0786681175231934\n",
      "2. Current minimum value loss = 0.6871075630187988\n",
      "3. Current minimum value loss = 0.08130147308111191\n",
      "episode: 790, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 189.0\n",
      "1. Current minimum value loss = 1.0804259777069092\n",
      "2. Current minimum value loss = 0.675593376159668\n",
      "5. Current minimum value loss = 0.4445585310459137\n",
      "8. Current minimum value loss = 0.4385508596897125\n",
      "10. Current minimum value loss = 0.14832398295402527\n",
      "episode: 800, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 196.55\n",
      "1. Current minimum value loss = 0.6715432405471802\n",
      "2. Current minimum value loss = 0.25270673632621765\n",
      "4. Current minimum value loss = 0.15789499878883362\n",
      "episode: 810, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 192.55\n",
      "1. Current minimum value loss = 0.9754049777984619\n",
      "2. Current minimum value loss = 0.1990615427494049\n",
      "7. Current minimum value loss = 0.15343664586544037\n",
      "episode: 820, total mean reward among trajectories: 173.0, average_reward_among_trajectories: 184.85\n",
      "1. Current minimum value loss = 0.4794965386390686\n",
      "2. Current minimum value loss = 0.24492305517196655\n",
      "episode: 830, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 183.25\n",
      "1. Current minimum value loss = 0.4203110337257385\n",
      "2. Current minimum value loss = 0.31730663776397705\n",
      "5. Current minimum value loss = 0.23099234700202942\n",
      "episode: 840, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 197.15\n",
      "1. Current minimum value loss = 0.7856316566467285\n",
      "4. Current minimum value loss = 0.5403018593788147\n",
      "5. Current minimum value loss = 0.14289644360542297\n",
      "episode: 850, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 183.15\n",
      "1. Current minimum value loss = 0.6171668767929077\n",
      "2. Current minimum value loss = 0.564067006111145\n",
      "3. Current minimum value loss = 0.4973851144313812\n",
      "9. Current minimum value loss = 0.3095102608203888\n",
      "episode: 860, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 186.75\n",
      "1. Current minimum value loss = 0.5173707008361816\n",
      "6. Current minimum value loss = 0.3047565221786499\n",
      "11. Current minimum value loss = 0.19088968634605408\n",
      "episode: 870, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 186.2\n",
      "1. Current minimum value loss = 0.877444326877594\n",
      "2. Current minimum value loss = 0.7154418230056763\n",
      "3. Current minimum value loss = 0.3139970898628235\n",
      "episode: 880, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 189.4\n",
      "1. Current minimum value loss = 0.7900408506393433\n",
      "2. Current minimum value loss = 0.47790563106536865\n",
      "3. Current minimum value loss = 0.2506433129310608\n",
      "episode: 890, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 187.8\n",
      "1. Current minimum value loss = 1.5676190853118896\n",
      "2. Current minimum value loss = 0.43416541814804077\n",
      "7. Current minimum value loss = 0.4045632481575012\n",
      "11. Current minimum value loss = 0.25068172812461853\n",
      "episode: 900, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 192.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Current minimum value loss = 0.4018760919570923\n",
      "episode: 910, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 195.55\n",
      "1. Current minimum value loss = 0.5436416268348694\n",
      "2. Current minimum value loss = 0.46172505617141724\n",
      "6. Current minimum value loss = 0.3545370101928711\n",
      "9. Current minimum value loss = 0.303093820810318\n",
      "episode: 920, total mean reward among trajectories: 142.5, average_reward_among_trajectories: 178.95\n",
      "1. Current minimum value loss = 0.6506990194320679\n",
      "2. Current minimum value loss = 0.6214426755905151\n",
      "4. Current minimum value loss = 0.3609652519226074\n",
      "12. Current minimum value loss = 0.30122238397598267\n",
      "16. Current minimum value loss = 0.24899932742118835\n",
      "episode: 930, total mean reward among trajectories: 163.5, average_reward_among_trajectories: 189.3\n",
      "1. Current minimum value loss = 0.48664623498916626\n",
      "2. Current minimum value loss = 0.26894888281822205\n",
      "4. Current minimum value loss = 0.25877657532691956\n",
      "7. Current minimum value loss = 0.22801488637924194\n",
      "episode: 940, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 195.75\n",
      "1. Current minimum value loss = 0.8165031671524048\n",
      "2. Current minimum value loss = 0.6794658899307251\n",
      "8. Current minimum value loss = 0.5198241472244263\n",
      "10. Current minimum value loss = 0.4796261489391327\n",
      "11. Current minimum value loss = 0.41339462995529175\n",
      "12. Current minimum value loss = 0.146652489900589\n",
      "episode: 950, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 190.05\n",
      "1. Current minimum value loss = 0.46284186840057373\n",
      "2. Current minimum value loss = 0.22988943755626678\n",
      "episode: 960, total mean reward among trajectories: 184.0, average_reward_among_trajectories: 193.05\n",
      "1. Current minimum value loss = 0.4269006848335266\n",
      "8. Current minimum value loss = 0.08851417899131775\n",
      "episode: 970, total mean reward among trajectories: 151.0, average_reward_among_trajectories: 189.0\n",
      "1. Current minimum value loss = 0.6274397373199463\n",
      "3. Current minimum value loss = 0.47144466638565063\n",
      "7. Current minimum value loss = 0.38105297088623047\n",
      "8. Current minimum value loss = 0.28449615836143494\n",
      "11. Current minimum value loss = 0.21486088633537292\n",
      "episode: 980, total mean reward among trajectories: 192.0, average_reward_among_trajectories: 181.65\n",
      "1. Current minimum value loss = 0.3804595470428467\n",
      "2. Current minimum value loss = 0.12764999270439148\n",
      "episode: 990, total mean reward among trajectories: 180.5, average_reward_among_trajectories: 193.65\n",
      "1. Current minimum value loss = 0.8030606508255005\n",
      "2. Current minimum value loss = 0.6548041105270386\n",
      "4. Current minimum value loss = 0.5249963998794556\n",
      "5. Current minimum value loss = 0.4057483375072479\n",
      "8. Current minimum value loss = 0.21171745657920837\n",
      "episode: 1000, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 192.65\n",
      "1. Current minimum value loss = 0.7054134011268616\n",
      "2. Current minimum value loss = 0.3386523723602295\n",
      "episode: 1010, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 188.0\n",
      "1. Current minimum value loss = 0.3177037835121155\n",
      "episode: 1020, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 199.45\n",
      "1. Current minimum value loss = 0.7006992101669312\n",
      "3. Current minimum value loss = 0.374795138835907\n",
      "4. Current minimum value loss = 0.3659384548664093\n",
      "12. Current minimum value loss = 0.21021214127540588\n",
      "21. Current minimum value loss = 0.16937679052352905\n",
      "episode: 1030, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 186.35\n",
      "1. Current minimum value loss = 0.8207509517669678\n",
      "6. Current minimum value loss = 0.684302806854248\n",
      "9. Current minimum value loss = 0.2911125719547272\n",
      "episode: 1040, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 195.45\n",
      "1. Current minimum value loss = 1.0001415014266968\n",
      "2. Current minimum value loss = 0.555274248123169\n",
      "12. Current minimum value loss = 0.40325623750686646\n",
      "episode: 1050, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 187.35\n",
      "1. Current minimum value loss = 0.8587162494659424\n",
      "2. Current minimum value loss = 0.37937045097351074\n",
      "7. Current minimum value loss = 0.3525446057319641\n",
      "17. Current minimum value loss = 0.2430172562599182\n",
      "episode: 1060, total mean reward among trajectories: 155.0, average_reward_among_trajectories: 183.15\n",
      "1. Current minimum value loss = 1.3381385803222656\n",
      "2. Current minimum value loss = 0.8874502182006836\n",
      "3. Current minimum value loss = 0.6058523654937744\n",
      "13. Current minimum value loss = 0.4545724093914032\n",
      "18. Current minimum value loss = 0.27242597937583923\n",
      "episode: 1070, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 189.5\n",
      "1. Current minimum value loss = 0.5824238657951355\n",
      "4. Current minimum value loss = 0.4248262345790863\n",
      "episode: 1080, total mean reward among trajectories: 199.5, average_reward_among_trajectories: 199.95\n",
      "1. Current minimum value loss = 0.5907498002052307\n",
      "4. Current minimum value loss = 0.42536622285842896\n",
      "episode: 1090, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 200.0\n",
      "1. Current minimum value loss = 1.4360041618347168\n",
      "2. Current minimum value loss = 0.9934253692626953\n",
      "3. Current minimum value loss = 0.8374258875846863\n",
      "5. Current minimum value loss = 0.2390744984149933\n",
      "episode: 1100, total mean reward among trajectories: 173.0, average_reward_among_trajectories: 196.15\n",
      "1. Current minimum value loss = 1.2173197269439697\n",
      "2. Current minimum value loss = 0.5850683450698853\n",
      "8. Current minimum value loss = 0.4896646738052368\n",
      "9. Current minimum value loss = 0.47339940071105957\n",
      "episode: 1110, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 192.35\n",
      "1. Current minimum value loss = 0.5157241225242615\n",
      "8. Current minimum value loss = 0.4849088788032532\n",
      "episode: 1120, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 200.0\n",
      "1. Current minimum value loss = 0.7364777326583862\n",
      "3. Current minimum value loss = 0.5738102197647095\n",
      "8. Current minimum value loss = 0.5106461048126221\n",
      "10. Current minimum value loss = 0.16226284205913544\n",
      "episode: 1130, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 200.0\n",
      "1. Current minimum value loss = 0.7633987665176392\n",
      "2. Current minimum value loss = 0.5169478058815002\n",
      "4. Current minimum value loss = 0.4213980436325073\n",
      "7. Current minimum value loss = 0.33843761682510376\n",
      "episode: 1140, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 194.25\n",
      "1. Current minimum value loss = 1.06504487991333\n",
      "3. Current minimum value loss = 0.5361219644546509\n",
      "5. Current minimum value loss = 0.3970823884010315\n",
      "11. Current minimum value loss = 0.38531801104545593\n",
      "15. Current minimum value loss = 0.2066875696182251\n",
      "episode: 1150, total mean reward among trajectories: 188.0, average_reward_among_trajectories: 194.2\n",
      "1. Current minimum value loss = 1.1007298231124878\n",
      "2. Current minimum value loss = 0.920106828212738\n",
      "3. Current minimum value loss = 0.7241745591163635\n",
      "4. Current minimum value loss = 0.706518292427063\n",
      "5. Current minimum value loss = 0.5635043382644653\n",
      "9. Current minimum value loss = 0.5198425054550171\n",
      "10. Current minimum value loss = 0.2959299385547638\n",
      "11. Current minimum value loss = 0.29182013869285583\n",
      "episode: 1160, total mean reward among trajectories: 186.5, average_reward_among_trajectories: 190.2\n",
      "1. Current minimum value loss = 0.8623296618461609\n",
      "2. Current minimum value loss = 0.4857726991176605\n",
      "4. Current minimum value loss = 0.22005227208137512\n",
      "episode: 1170, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 187.25\n",
      "1. Current minimum value loss = 0.5394538044929504\n",
      "2. Current minimum value loss = 0.5279351472854614\n",
      "3. Current minimum value loss = 0.2858971953392029\n",
      "11. Current minimum value loss = 0.2846316695213318\n",
      "episode: 1180, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 199.4\n",
      "1. Current minimum value loss = 0.3722854256629944\n",
      "10. Current minimum value loss = 0.2447580099105835\n",
      "episode: 1190, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 190.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Current minimum value loss = 0.5461580753326416\n",
      "6. Current minimum value loss = 0.5236278772354126\n",
      "12. Current minimum value loss = 0.2827530801296234\n",
      "episode: 1200, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 198.05\n",
      "1. Current minimum value loss = 1.1089508533477783\n",
      "2. Current minimum value loss = 0.49951171875\n",
      "3. Current minimum value loss = 0.4975201189517975\n",
      "4. Current minimum value loss = 0.30417436361312866\n",
      "episode: 1210, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 196.7\n",
      "1. Current minimum value loss = 0.7667005658149719\n",
      "5. Current minimum value loss = 0.3957759737968445\n",
      "7. Current minimum value loss = 0.38262268900871277\n",
      "episode: 1220, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 194.3\n",
      "1. Current minimum value loss = 1.2270561456680298\n",
      "2. Current minimum value loss = 0.7801154255867004\n",
      "3. Current minimum value loss = 0.44167041778564453\n",
      "12. Current minimum value loss = 0.27558785676956177\n",
      "episode: 1230, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 200.0\n",
      "1. Current minimum value loss = 0.6403043866157532\n",
      "7. Current minimum value loss = 0.6340638399124146\n",
      "10. Current minimum value loss = 0.31117480993270874\n",
      "19. Current minimum value loss = 0.31052377820014954\n",
      "episode: 1240, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 194.4\n",
      "1. Current minimum value loss = 0.5949602723121643\n",
      "3. Current minimum value loss = 0.5902851819992065\n",
      "4. Current minimum value loss = 0.39717233180999756\n",
      "7. Current minimum value loss = 0.20235109329223633\n",
      "episode: 1250, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 186.65\n",
      "1. Current minimum value loss = 1.239185094833374\n",
      "2. Current minimum value loss = 0.3902164101600647\n",
      "10. Current minimum value loss = 0.34405797719955444\n",
      "episode: 1260, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 192.45\n",
      "1. Current minimum value loss = 0.8859007358551025\n",
      "2. Current minimum value loss = 0.7549542188644409\n",
      "3. Current minimum value loss = 0.3092905282974243\n",
      "episode: 1270, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 189.05\n",
      "1. Current minimum value loss = 0.7493754625320435\n",
      "2. Current minimum value loss = 0.33867400884628296\n",
      "episode: 1280, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 198.75\n",
      "1. Current minimum value loss = 0.6933813691139221\n",
      "4. Current minimum value loss = 0.6706836223602295\n",
      "5. Current minimum value loss = 0.6552836894989014\n",
      "10. Current minimum value loss = 0.37625443935394287\n",
      "18. Current minimum value loss = 0.27949637174606323\n",
      "20. Current minimum value loss = 0.2443753480911255\n",
      "21. Current minimum value loss = 0.23932042717933655\n",
      "episode: 1290, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 198.35\n",
      "1. Current minimum value loss = 1.8051310777664185\n",
      "2. Current minimum value loss = 0.4066244661808014\n",
      "4. Current minimum value loss = 0.3106217384338379\n",
      "episode: 1300, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 196.3\n",
      "1. Current minimum value loss = 0.8852994441986084\n",
      "2. Current minimum value loss = 0.6910058259963989\n",
      "5. Current minimum value loss = 0.2995891571044922\n",
      "episode: 1310, total mean reward among trajectories: 162.0, average_reward_among_trajectories: 193.8\n",
      "1. Current minimum value loss = 1.123661994934082\n",
      "2. Current minimum value loss = 0.6666537523269653\n",
      "3. Current minimum value loss = 0.3703573942184448\n",
      "7. Current minimum value loss = 0.29894596338272095\n",
      "9. Current minimum value loss = 0.24749913811683655\n",
      "episode: 1320, total mean reward among trajectories: 165.5, average_reward_among_trajectories: 196.55\n",
      "1. Current minimum value loss = 1.6938400268554688\n",
      "2. Current minimum value loss = 0.334530770778656\n",
      "5. Current minimum value loss = 0.27683112025260925\n",
      "episode: 1330, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 200.0\n",
      "1. Current minimum value loss = 0.8212989568710327\n",
      "3. Current minimum value loss = 0.30950498580932617\n",
      "episode: 1340, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 195.95\n",
      "1. Current minimum value loss = 0.7961558699607849\n",
      "4. Current minimum value loss = 0.7697434425354004\n",
      "5. Current minimum value loss = 0.6351382732391357\n",
      "7. Current minimum value loss = 0.5403209924697876\n",
      "episode: 1350, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 188.7\n",
      "1. Current minimum value loss = 0.7531004548072815\n",
      "3. Current minimum value loss = 0.7109518051147461\n",
      "4. Current minimum value loss = 0.37565839290618896\n",
      "13. Current minimum value loss = 0.24105019867420197\n",
      "episode: 1360, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 193.8\n",
      "1. Current minimum value loss = 0.5832637548446655\n",
      "4. Current minimum value loss = 0.37781432271003723\n",
      "7. Current minimum value loss = 0.3364483714103699\n",
      "episode: 1370, total mean reward among trajectories: 175.0, average_reward_among_trajectories: 194.8\n",
      "1. Current minimum value loss = 0.8748896718025208\n",
      "3. Current minimum value loss = 0.7002111673355103\n",
      "4. Current minimum value loss = 0.6421881914138794\n",
      "14. Current minimum value loss = 0.6230627298355103\n",
      "23. Current minimum value loss = 0.4067137837409973\n",
      "episode: 1380, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 191.5\n",
      "1. Current minimum value loss = 0.8131864666938782\n",
      "5. Current minimum value loss = 0.7343609929084778\n",
      "7. Current minimum value loss = 0.4108577370643616\n",
      "episode: 1390, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 192.45\n",
      "1. Current minimum value loss = 0.7615392208099365\n",
      "3. Current minimum value loss = 0.6467424035072327\n",
      "11. Current minimum value loss = 0.3024700880050659\n",
      "episode: 1400, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 200.0\n",
      "1. Current minimum value loss = 0.7187202572822571\n",
      "8. Current minimum value loss = 0.717124342918396\n",
      "10. Current minimum value loss = 0.6251097917556763\n",
      "16. Current minimum value loss = 0.6188480854034424\n",
      "19. Current minimum value loss = 0.48272430896759033\n",
      "27. Current minimum value loss = 0.41136348247528076\n",
      "31. Current minimum value loss = 0.30700188875198364\n",
      "episode: 1410, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 199.1\n",
      "1. Current minimum value loss = 0.9844602346420288\n",
      "2. Current minimum value loss = 0.5363661050796509\n",
      "episode: 1420, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 197.15\n",
      "1. Current minimum value loss = 1.1665074825286865\n",
      "3. Current minimum value loss = 0.4597753882408142\n",
      "8. Current minimum value loss = 0.44709745049476624\n",
      "episode: 1430, total mean reward among trajectories: 168.5, average_reward_among_trajectories: 188.8\n",
      "1. Current minimum value loss = 1.5940802097320557\n",
      "2. Current minimum value loss = 0.7628363370895386\n",
      "5. Current minimum value loss = 0.22660037875175476\n",
      "episode: 1440, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 193.75\n",
      "1. Current minimum value loss = 0.8163101673126221\n",
      "2. Current minimum value loss = 0.5584667921066284\n",
      "5. Current minimum value loss = 0.3962286114692688\n",
      "9. Current minimum value loss = 0.26970529556274414\n",
      "episode: 1450, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 187.75\n",
      "1. Current minimum value loss = 0.8729729652404785\n",
      "3. Current minimum value loss = 0.3125278949737549\n",
      "episode: 1460, total mean reward among trajectories: 188.0, average_reward_among_trajectories: 195.15\n",
      "1. Current minimum value loss = 1.3876605033874512\n",
      "2. Current minimum value loss = 0.6498932242393494\n",
      "5. Current minimum value loss = 0.6218066811561584\n",
      "8. Current minimum value loss = 0.5340291857719421\n",
      "15. Current minimum value loss = 0.18155261874198914\n",
      "episode: 1470, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 191.15\n",
      "1. Current minimum value loss = 0.5376185178756714\n",
      "3. Current minimum value loss = 0.39996498823165894\n",
      "episode: 1480, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 189.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Current minimum value loss = 0.29491108655929565\n",
      "episode: 1490, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 195.6\n",
      "1. Current minimum value loss = 0.586611270904541\n",
      "5. Current minimum value loss = 0.22474166750907898\n",
      "episode: 1500, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 190.35\n",
      "1. Current minimum value loss = 0.647983968257904\n",
      "5. Current minimum value loss = 0.5791577696800232\n",
      "7. Current minimum value loss = 0.41652029752731323\n",
      "9. Current minimum value loss = 0.24060362577438354\n",
      "episode: 1510, total mean reward among trajectories: 199.0, average_reward_among_trajectories: 197.4\n",
      "1. Current minimum value loss = 0.8676257133483887\n",
      "2. Current minimum value loss = 0.28906917572021484\n",
      "3. Current minimum value loss = 0.2155231237411499\n",
      "episode: 1520, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 196.3\n",
      "1. Current minimum value loss = 0.9800758361816406\n",
      "2. Current minimum value loss = 0.7264062166213989\n",
      "6. Current minimum value loss = 0.7046850919723511\n",
      "7. Current minimum value loss = 0.31208133697509766\n",
      "episode: 1530, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 189.95\n",
      "1. Current minimum value loss = 0.6524733304977417\n",
      "4. Current minimum value loss = 0.23320895433425903\n",
      "episode: 1540, total mean reward among trajectories: 157.5, average_reward_among_trajectories: 193.0\n",
      "1. Current minimum value loss = 0.5582801103591919\n",
      "6. Current minimum value loss = 0.48204106092453003\n",
      "13. Current minimum value loss = 0.30324721336364746\n",
      "episode: 1550, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 193.2\n",
      "1. Current minimum value loss = 0.5046602487564087\n",
      "3. Current minimum value loss = 0.37402644753456116\n",
      "8. Current minimum value loss = 0.27213597297668457\n",
      "episode: 1560, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 200.0\n",
      "1. Current minimum value loss = 0.6950916051864624\n",
      "4. Current minimum value loss = 0.30377674102783203\n",
      "8. Current minimum value loss = 0.2986912131309509\n",
      "11. Current minimum value loss = 0.2856156826019287\n",
      "12. Current minimum value loss = 0.2662523090839386\n",
      "episode: 1570, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 197.05\n",
      "1. Current minimum value loss = 0.49863430857658386\n",
      "4. Current minimum value loss = 0.3584553003311157\n",
      "11. Current minimum value loss = 0.3581758737564087\n",
      "episode: 1580, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 199.3\n",
      "1. Current minimum value loss = 1.1905670166015625\n",
      "2. Current minimum value loss = 0.31024566292762756\n",
      "episode: 1590, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 200.0\n",
      "1. Current minimum value loss = 0.5080001950263977\n",
      "episode: 1600, total mean reward among trajectories: 183.0, average_reward_among_trajectories: 198.3\n",
      "1. Current minimum value loss = 0.3683139979839325\n",
      "4. Current minimum value loss = 0.27956241369247437\n",
      "episode: 1610, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 194.75\n",
      "1. Current minimum value loss = 0.7090603709220886\n",
      "3. Current minimum value loss = 0.33295196294784546\n",
      "7. Current minimum value loss = 0.09073631465435028\n",
      "episode: 1620, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 191.85\n",
      "1. Current minimum value loss = 0.512557864189148\n",
      "4. Current minimum value loss = 0.37666669487953186\n",
      "7. Current minimum value loss = 0.3264222741127014\n",
      "episode: 1630, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 198.0\n",
      "1. Current minimum value loss = 1.172709584236145\n",
      "2. Current minimum value loss = 0.7060011625289917\n",
      "4. Current minimum value loss = 0.58065265417099\n",
      "5. Current minimum value loss = 0.4212598204612732\n",
      "6. Current minimum value loss = 0.41423046588897705\n",
      "14. Current minimum value loss = 0.2172013223171234\n",
      "episode: 1640, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 197.45\n",
      "1. Current minimum value loss = 0.6554638147354126\n",
      "2. Current minimum value loss = 0.6402223706245422\n",
      "3. Current minimum value loss = 0.4842156767845154\n",
      "6. Current minimum value loss = 0.229737788438797\n",
      "episode: 1650, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 199.4\n",
      "1. Current minimum value loss = 0.5892068147659302\n",
      "2. Current minimum value loss = 0.22559574246406555\n",
      "episode: 1660, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 200.0\n",
      "1. Current minimum value loss = 0.9424517750740051\n",
      "2. Current minimum value loss = 0.7858527898788452\n",
      "3. Current minimum value loss = 0.6253080368041992\n",
      "4. Current minimum value loss = 0.5940588116645813\n",
      "5. Current minimum value loss = 0.4213342070579529\n",
      "6. Current minimum value loss = 0.38061386346817017\n",
      "episode: 1670, total mean reward among trajectories: 187.5, average_reward_among_trajectories: 194.25\n",
      "1. Current minimum value loss = 0.32081156969070435\n",
      "episode: 1680, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 200.0\n",
      "1. Current minimum value loss = 0.8140966892242432\n",
      "2. Current minimum value loss = 0.7255433201789856\n",
      "4. Current minimum value loss = 0.569496750831604\n",
      "7. Current minimum value loss = 0.254663348197937\n",
      "episode: 1690, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 194.3\n",
      "1. Current minimum value loss = 0.6128417253494263\n",
      "2. Current minimum value loss = 0.47835567593574524\n",
      "4. Current minimum value loss = 0.31990498304367065\n",
      "8. Current minimum value loss = 0.20514044165611267\n",
      "episode: 1700, total mean reward among trajectories: 173.5, average_reward_among_trajectories: 186.0\n",
      "1. Current minimum value loss = 0.27236610651016235\n",
      "3. Current minimum value loss = 0.19325454533100128\n",
      "episode: 1710, total mean reward among trajectories: 193.5, average_reward_among_trajectories: 190.15\n",
      "1. Current minimum value loss = 0.954647421836853\n",
      "2. Current minimum value loss = 0.3834177851676941\n",
      "5. Current minimum value loss = 0.3602304458618164\n",
      "13. Current minimum value loss = 0.1897241324186325\n",
      "episode: 1720, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 197.6\n",
      "1. Current minimum value loss = 0.6742785573005676\n",
      "2. Current minimum value loss = 0.45414191484451294\n",
      "8. Current minimum value loss = 0.42915594577789307\n",
      "14. Current minimum value loss = 0.3942837417125702\n",
      "16. Current minimum value loss = 0.18537524342536926\n",
      "25. Current minimum value loss = 0.18404357135295868\n",
      "episode: 1730, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 200.0\n",
      "1. Current minimum value loss = 0.7584969997406006\n",
      "5. Current minimum value loss = 0.71149742603302\n",
      "11. Current minimum value loss = 0.6254260540008545\n",
      "12. Current minimum value loss = 0.5188950300216675\n",
      "15. Current minimum value loss = 0.49310556054115295\n",
      "16. Current minimum value loss = 0.3519849181175232\n",
      "25. Current minimum value loss = 0.33169543743133545\n",
      "29. Current minimum value loss = 0.24042513966560364\n",
      "episode: 1740, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 200.0\n",
      "1. Current minimum value loss = 0.3115643560886383\n",
      "3. Current minimum value loss = 0.2557843327522278\n",
      "8. Current minimum value loss = 0.18773934245109558\n",
      "episode: 1750, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 200.0\n",
      "1. Current minimum value loss = 0.6811046004295349\n",
      "2. Current minimum value loss = 0.5427802801132202\n",
      "3. Current minimum value loss = 0.36653226613998413\n",
      "6. Current minimum value loss = 0.2691647708415985\n",
      "episode: 1760, total mean reward among trajectories: 191.5, average_reward_among_trajectories: 198.55\n",
      "1. Current minimum value loss = 0.7817322015762329\n",
      "4. Current minimum value loss = 0.4934394359588623\n",
      "13. Current minimum value loss = 0.39979425072669983\n",
      "22. Current minimum value loss = 0.26240965723991394\n",
      "23. Current minimum value loss = 0.20964162051677704\n",
      "25. Current minimum value loss = 0.19781967997550964\n",
      "28. Current minimum value loss = 0.15985070168972015\n",
      "episode: 1770, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 200.0\n",
      "1. Current minimum value loss = 0.7828121185302734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Current minimum value loss = 0.4659367799758911\n",
      "7. Current minimum value loss = 0.3893349766731262\n",
      "17. Current minimum value loss = 0.32998672127723694\n",
      "21. Current minimum value loss = 0.2542041540145874\n",
      "episode: 1780, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 200.0\n",
      "1. Current minimum value loss = 0.901498019695282\n",
      "3. Current minimum value loss = 0.8469089865684509\n",
      "5. Current minimum value loss = 0.8027751445770264\n",
      "6. Current minimum value loss = 0.10300751030445099\n",
      "episode: 1790, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 192.8\n",
      "1. Current minimum value loss = 0.9786896109580994\n",
      "2. Current minimum value loss = 0.27304190397262573\n",
      "6. Current minimum value loss = 0.26856085658073425\n",
      "episode: 1800, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 195.25\n",
      "1. Current minimum value loss = 0.6017053723335266\n",
      "2. Current minimum value loss = 0.4797601103782654\n",
      "5. Current minimum value loss = 0.34093207120895386\n",
      "episode: 1810, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 198.9\n",
      "1. Current minimum value loss = 1.2242087125778198\n",
      "2. Current minimum value loss = 0.9110615253448486\n",
      "4. Current minimum value loss = 0.7289316058158875\n",
      "6. Current minimum value loss = 0.6981168389320374\n",
      "7. Current minimum value loss = 0.39157265424728394\n",
      "11. Current minimum value loss = 0.3767262101173401\n",
      "episode: 1820, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 198.95\n",
      "1. Current minimum value loss = 0.6130715012550354\n",
      "2. Current minimum value loss = 0.4470130205154419\n",
      "6. Current minimum value loss = 0.2341831624507904\n",
      "episode: 1830, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 200.0\n",
      "1. Current minimum value loss = 0.47334393858909607\n",
      "5. Current minimum value loss = 0.46943819522857666\n",
      "8. Current minimum value loss = 0.3513363003730774\n",
      "15. Current minimum value loss = 0.10848341882228851\n",
      "episode: 1840, total mean reward among trajectories: 172.0, average_reward_among_trajectories: 197.2\n",
      "1. Current minimum value loss = 0.42092132568359375\n",
      "6. Current minimum value loss = 0.20555302500724792\n",
      "episode: 1850, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 200.0\n",
      "1. Current minimum value loss = 0.5899756550788879\n",
      "2. Current minimum value loss = 0.46934807300567627\n",
      "5. Current minimum value loss = 0.21683497726917267\n",
      "episode: 1860, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 196.85\n",
      "1. Current minimum value loss = 0.5966281890869141\n",
      "2. Current minimum value loss = 0.5749427676200867\n",
      "6. Current minimum value loss = 0.5247575044631958\n",
      "10. Current minimum value loss = 0.4564341902732849\n",
      "13. Current minimum value loss = 0.4038570523262024\n",
      "16. Current minimum value loss = 0.16044355928897858\n",
      "episode: 1870, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 198.65\n",
      "1. Current minimum value loss = 0.7005251049995422\n",
      "2. Current minimum value loss = 0.5074783563613892\n",
      "4. Current minimum value loss = 0.4280385673046112\n",
      "7. Current minimum value loss = 0.3318237066268921\n",
      "10. Current minimum value loss = 0.2873739004135132\n",
      "19. Current minimum value loss = 0.28189152479171753\n",
      "episode: 1880, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 198.85\n",
      "1. Current minimum value loss = 0.8043088912963867\n",
      "2. Current minimum value loss = 0.4975593090057373\n",
      "3. Current minimum value loss = 0.29802748560905457\n",
      "episode: 1890, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 196.45\n",
      "1. Current minimum value loss = 0.933286726474762\n",
      "2. Current minimum value loss = 0.566106915473938\n",
      "8. Current minimum value loss = 0.44238191843032837\n",
      "13. Current minimum value loss = 0.3699955344200134\n",
      "16. Current minimum value loss = 0.33593034744262695\n",
      "21. Current minimum value loss = 0.31030330061912537\n",
      "30. Current minimum value loss = 0.23770904541015625\n",
      "31. Current minimum value loss = 0.20196405053138733\n",
      "episode: 1900, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 189.35\n",
      "1. Current minimum value loss = 0.6569756269454956\n",
      "3. Current minimum value loss = 0.4295664429664612\n",
      "5. Current minimum value loss = 0.38096535205841064\n",
      "episode: 1910, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 200.0\n",
      "1. Current minimum value loss = 0.15636414289474487\n",
      "episode: 1920, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 198.9\n",
      "1. Current minimum value loss = 0.8644605278968811\n",
      "2. Current minimum value loss = 0.7136843204498291\n",
      "4. Current minimum value loss = 0.6339487433433533\n",
      "5. Current minimum value loss = 0.45033717155456543\n",
      "10. Current minimum value loss = 0.3748199939727783\n",
      "13. Current minimum value loss = 0.26633790135383606\n",
      "18. Current minimum value loss = 0.23696789145469666\n",
      "episode: 1930, total mean reward among trajectories: 198.0, average_reward_among_trajectories: 197.8\n",
      "1. Current minimum value loss = 0.3455083668231964\n",
      "5. Current minimum value loss = 0.31590133905410767\n",
      "9. Current minimum value loss = 0.1841994971036911\n",
      "episode: 1940, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 191.5\n",
      "1. Current minimum value loss = 0.8542757034301758\n",
      "2. Current minimum value loss = 0.73055100440979\n",
      "3. Current minimum value loss = 0.34214842319488525\n",
      "4. Current minimum value loss = 0.32112917304039\n",
      "9. Current minimum value loss = 0.2548445463180542\n",
      "11. Current minimum value loss = 0.20951935648918152\n",
      "episode: 1950, total mean reward among trajectories: 189.0, average_reward_among_trajectories: 195.4\n",
      "1. Current minimum value loss = 0.47474104166030884\n",
      "5. Current minimum value loss = 0.22575023770332336\n",
      "8. Current minimum value loss = 0.2148015797138214\n",
      "9. Current minimum value loss = 0.16395190358161926\n",
      "episode: 1960, total mean reward among trajectories: 187.0, average_reward_among_trajectories: 196.85\n",
      "1. Current minimum value loss = 0.4028966426849365\n",
      "5. Current minimum value loss = 0.27693822979927063\n",
      "episode: 1970, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 196.5\n",
      "1. Current minimum value loss = 0.6403928995132446\n",
      "2. Current minimum value loss = 0.44904404878616333\n",
      "6. Current minimum value loss = 0.3409996032714844\n",
      "7. Current minimum value loss = 0.28957322239875793\n",
      "episode: 1980, total mean reward among trajectories: 190.5, average_reward_among_trajectories: 191.65\n",
      "1. Current minimum value loss = 1.1577589511871338\n",
      "2. Current minimum value loss = 0.6427910327911377\n",
      "3. Current minimum value loss = 0.3579157888889313\n",
      "6. Current minimum value loss = 0.29320627450942993\n",
      "8. Current minimum value loss = 0.27014777064323425\n",
      "18. Current minimum value loss = 0.21658939123153687\n",
      "episode: 1990, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 193.65\n",
      "1. Current minimum value loss = 1.0306419134140015\n",
      "2. Current minimum value loss = 0.5360866785049438\n",
      "7. Current minimum value loss = 0.22305472195148468\n",
      "episode: 2000, total mean reward among trajectories: 200.0, average_reward_among_trajectories: 196.05\n",
      "1. Current minimum value loss = 0.1633937805891037\n",
      "Wall time: 58min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_rewards, mean_rewards = cart_pole_baseline(value_net, 2, 2000, True, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Varince of reward = 3354.229025\n"
     ]
    }
   ],
   "source": [
    "plt.plot(np.mean(all_rewards, axis = 0))\n",
    "plt.plot(np.mean(mean_rewards, axis = 0))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()\n",
    "print('Varince of reward = {}'.format(np.var(np.mean(all_rewards, axis = 0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['rewards'] = np.mean(all_rewards, axis = 0)\n",
    "df['mean10_rewards'] = np.mean(mean_rewards, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('D:/Reinforcement_Learning/df_reinforce_2_trajectories_baseline_2000_retrain.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
