{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement_learning_baseline_averaged.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "av4t4b5g45el",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import torch  \n",
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from scipy.optimize import least_squares"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s17K4twqNjBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.tensor([1.0], requires_grad=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1IEt8dQNnKp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = x + 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uyGPkxcNowM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "97ab2384-bece-4451-91e8-779ce7071aaf"
      },
      "source": [
        "x"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3.], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTF0WguK-F70",
        "colab_type": "code",
        "outputId": "b9004432-d4b1-4932-a172-83037c6861a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFP-x3MA5LEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Constants\n",
        "GAMMA = 0.99\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, num_inputs, num_actions, hidden_size, learning_rate=3e-4):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "\n",
        "        self.num_actions = num_actions\n",
        "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, num_actions)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.linear1(state))\n",
        "        x = F.softmax(self.linear2(x), dim=1)\n",
        "        return x \n",
        "    \n",
        "    def get_action(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "        probs = self.forward(Variable(state))\n",
        "        #Choose action with regard to policy\n",
        "        highest_prob_action = np.random.choice(self.num_actions, p=np.squeeze(probs.detach().numpy()))\n",
        "        log_prob = torch.log(probs.squeeze(0)[highest_prob_action]) #log for gradient\n",
        "        return highest_prob_action, log_prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK5vloYO5Ox5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, num_inputs, num_actions, hidden_size, learning_rate=3e-4):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "\n",
        "        self.num_actions = num_actions\n",
        "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, 1)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "    \n",
        "    def forward(self, state):\n",
        "        state = Variable(torch.from_numpy(state).float().unsqueeze(0))\n",
        "        value = F.relu(self.linear1(state))\n",
        "        value = self.linear2(value)\n",
        "        return value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fH6RMdJm5Rao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_policy(policy_network, trajectories_gradient):\n",
        "    policy_network.optimizer.zero_grad()    \n",
        "    policy_gradient = torch.stack(trajectories_gradient).sum()\n",
        "    policy_gradient.backward()\n",
        "    policy_network.optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1wZxyEv-A_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_trajectory_for_baseline(rewards, log_probs, baseline):\n",
        "    discounted_rewards = []\n",
        "\n",
        "    for t in range(len(rewards)):\n",
        "        Gt = 0 \n",
        "        pw = 0\n",
        "        for r in rewards[t:]:\n",
        "            Gt = Gt + GAMMA**pw * r\n",
        "            pw = pw + 1\n",
        "        discounted_rewards.append(Gt)\n",
        "        \n",
        "    discounted_rewards = torch.tensor(discounted_rewards)\n",
        "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9) # normalize discounted rewards\n",
        "\n",
        "    policy_gradient = []\n",
        "    log_deriv = []\n",
        "    for log_prob in log_probs:\n",
        "      if log_prob != 0:\n",
        "        log_prob.backward()\n",
        "        deriv = []\n",
        "        for param in policy_net.parameters():\n",
        "          deriv.append(param.grad.resize(param.grad.numel())**2)\n",
        "        deriv = torch.cat([tensor for tensor in deriv], 0)\n",
        "        log_deriv.append(deriv.sum())\n",
        "      else:\n",
        "        log_deriv.append(0)\n",
        "\n",
        "    for log_prob_deriv, Gt, bs in zip(log_deriv, discounted_rewards, baseline):\n",
        "        if log_prob_deriv == 0:\n",
        "            policy_gradient.append(torch.tensor([0.0], requires_grad = True))\n",
        "        else:\n",
        "            term = torch.tensor([log_prob_deriv], requires_grad = True)\n",
        "            term = term * (Gt - bs.resize(1))**2\n",
        "            #policy_gradient.append(torch.tensor([-log_prob_deriv * (Gt - bs)], requires_grad = True))\n",
        "            policy_gradient.append(term)\n",
        "    policy_gradient = torch.stack(policy_gradient).sum()\n",
        "    return policy_gradient"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K9WsvOe5UF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_trajectory_for_policy(rewards, log_probs, baseline):\n",
        "    discounted_rewards = []\n",
        "\n",
        "    for t in range(len(rewards)):\n",
        "        Gt = 0 \n",
        "        pw = 0\n",
        "        for r in rewards[t:]:\n",
        "            Gt = Gt + GAMMA**pw * r\n",
        "            pw = pw + 1\n",
        "        discounted_rewards.append(Gt)\n",
        "        \n",
        "    discounted_rewards = torch.tensor(discounted_rewards)\n",
        "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9) # normalize discounted rewards\n",
        "\n",
        "    policy_gradient = []\n",
        "    for log_prob, Gt, bs in zip(log_probs, discounted_rewards, baseline):\n",
        "        if log_prob == 0:\n",
        "            policy_gradient.append(torch.tensor([[0.0]], requires_grad = True))\n",
        "        else:\n",
        "            policy_gradient.append(-log_prob * (Gt - bs))\n",
        "    \n",
        "    policy_gradient = torch.stack(policy_gradient).sum()\n",
        "    return policy_gradient"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4LCCInY5Xvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def align_columns_zeros(array):\n",
        "    max_length = max(list(map(lambda x: len(x), array)))\n",
        "    for col in range(len(array)):\n",
        "        array[col] = np.pad(array[col], max_length - len(array[col]), 'constant', constant_values = 0)[max_length - len(array[col]):]\n",
        "    return array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLm4i1Vh5b2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def baseline_approximation(policy_net, value_net, n_trajectories, n_epoch = 4000, early_stopping_rounds = 250):\n",
        "    max_steps = 10000\n",
        "    min_loss = float('inf')\n",
        "    stopping_rounds = 0\n",
        "    epoch = 1\n",
        "    while(stopping_rounds < early_stopping_rounds and epoch < n_epoch):\n",
        "        r_gradient = []\n",
        "        rewards = [[] for i in range(n_trajectories)]\n",
        "        log_probs = [[] for i in range(n_trajectories)]\n",
        "        baseline_values = [[] for i in range(n_trajectories)]\n",
        "    \n",
        "        for trajectory in range(n_trajectories):\n",
        "            state = env.reset()\n",
        "            \n",
        "            for steps in range(max_steps):\n",
        "                baseline = value_net.forward(state)\n",
        "                action, log_prob = policy_net.get_action(state)\n",
        "                new_state, reward, done, _ = env.step(np.array(action))\n",
        "                baseline_values[trajectory].append(baseline)\n",
        "                log_probs[trajectory].append(log_prob)\n",
        "                rewards[trajectory].append(reward)\n",
        "                if done:                    \n",
        "                    break\n",
        "                state = new_state\n",
        "        rewards = align_columns_zeros(rewards)\n",
        "        log_probs = align_columns_zeros(log_probs)\n",
        "        baseline_values = align_columns_zeros(baseline_values)\n",
        "        for col in range(len(rewards)):\n",
        "            traj = count_trajectory_for_baseline(rewards[col], log_probs[col], baseline_values[col])\n",
        "            r_gradient.append(traj)\n",
        "\n",
        "        r_gradient = torch.stack(r_gradient)\n",
        "        value_loss = r_gradient.mean()\n",
        "        if value_loss < min_loss:\n",
        "            min_loss = value_loss\n",
        "            torch.save(value_net, 'model.pth')\n",
        "            #print('{}. Current minimum value loss = {}'.format(epoch, value_loss))\n",
        "            stopping_rounds = 0\n",
        "        else:\n",
        "            stopping_rounds += 1\n",
        "        print('{}. Current value loss = {}, min value_loss = {}, stopping_round = {}'.format(epoch, value_loss, min_loss, stopping_rounds))\n",
        "        epoch += 1\n",
        "        value_net.optimizer.zero_grad()\n",
        "        value_loss.backward()\n",
        "        value_net.optimizer.step()\n",
        "    value_net = torch.load('model.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuJiO0Tm5ifD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cart_pole_baseline(value_net, n_trajectories = 2, episode_num = 1500, baseline_retrain = False, retrain_episodes = 50):\n",
        "    env = gym.make('CartPole-v0')\n",
        "    policy_net = PolicyNetwork(env.observation_space.shape[0], env.action_space.n, 256)\n",
        "    \n",
        "\n",
        "    max_episode_num = episode_num\n",
        "    max_steps = 10000\n",
        "    numsteps = [[] for i in range(n_trajectories)]\n",
        "    avg_numsteps = [[] for i in range(n_trajectories)]\n",
        "    all_rewards = [[] for i in range(n_trajectories)]\n",
        "\n",
        "    for episode in range(1, max_episode_num + 1):\n",
        "        r_gradient = []\n",
        "        rewards = [[] for i in range(n_trajectories)]\n",
        "        baseline_values = []\n",
        "        \n",
        "        for trajectory in range(n_trajectories):\n",
        "            state = env.reset()\n",
        "            log_probs = []\n",
        "            \n",
        "            for steps in range(max_steps):\n",
        "                #env.render()\n",
        "                baseline = value_net.forward(state)\n",
        "                action, log_prob = policy_net.get_action(state)\n",
        "                new_state, reward, done, _ = env.step(action)\n",
        "                log_probs.append(log_prob)\n",
        "                rewards[trajectory].append(reward)\n",
        "                baseline_values.append(baseline)\n",
        "                \n",
        "                if done:\n",
        "                    traj = count_trajectory_for_policy(rewards[trajectory], log_probs, baseline_values)                       \n",
        "                    r_gradient.append(traj)\n",
        "                    numsteps[trajectory].append(steps)\n",
        "                    avg_numsteps[trajectory].append(np.mean(numsteps[trajectory][-10:]))\n",
        "                    all_rewards[trajectory].append(np.sum(rewards[trajectory]))\n",
        "                    break\n",
        "\n",
        "                state = new_state\n",
        "                \n",
        "        update_policy(policy_net, r_gradient)                 \n",
        "        rewards = align_columns_zeros(rewards)\n",
        "        #print(all_rewards)\n",
        "        if episode % 10 == 0:\n",
        "            sys.stdout.write(\"episode: {}, total mean reward among trajectories: {}, average_reward_among_trajectories: {}\\n\".\\\n",
        "                                     format(episode, np.round(np.mean(np.sum(rewards, axis = 1)), decimals = 3),\\\n",
        "                                            np.round(np.mean(np.mean(all_rewards, axis = 0)[-10:]), decimals = 3)))\n",
        "            \n",
        "        if baseline_retrain == True:\n",
        "            if episode % retrain_episodes == 0:\n",
        "                baseline_approximation(policy_net, value_net, 3, n_epoch = 1000, early_stopping_rounds=10)\n",
        "        \n",
        "    return all_rewards, avg_numsteps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-IAQduN5n25",
        "colab_type": "code",
        "outputId": "6881a3ee-09d1-4e37-89b1-bef0dbbe05e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "env = gym.make('CartPole-v0')\n",
        "policy_net = PolicyNetwork(env.observation_space.shape[0], env.action_space.n, 256)\n",
        "value_net = ValueNetwork(env.observation_space.shape[0], env.action_space.n, 256)\n",
        "baseline_approximation(policy_net, value_net, 15, early_stopping_rounds=150)"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
            "  warnings.warn(\"non-inplace resize is deprecated\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type ValueNetwork. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1. Current value loss = 48552.54296875, min value_loss = 48552.54296875, stopping_round = 0\n",
            "2. Current value loss = 56234.0234375, min value_loss = 48552.54296875, stopping_round = 1\n",
            "3. Current value loss = 202354.0, min value_loss = 48552.54296875, stopping_round = 2\n",
            "4. Current value loss = 61100.875, min value_loss = 48552.54296875, stopping_round = 3\n",
            "5. Current value loss = 75082.96875, min value_loss = 48552.54296875, stopping_round = 4\n",
            "6. Current value loss = 352075.78125, min value_loss = 48552.54296875, stopping_round = 5\n",
            "7. Current value loss = 483978.875, min value_loss = 48552.54296875, stopping_round = 6\n",
            "8. Current value loss = 845339.875, min value_loss = 48552.54296875, stopping_round = 7\n",
            "9. Current value loss = 1564431.25, min value_loss = 48552.54296875, stopping_round = 8\n",
            "10. Current value loss = 1602026.125, min value_loss = 48552.54296875, stopping_round = 9\n",
            "11. Current value loss = 2521939.25, min value_loss = 48552.54296875, stopping_round = 10\n",
            "12. Current value loss = 1367674.75, min value_loss = 48552.54296875, stopping_round = 11\n",
            "13. Current value loss = 948808.75, min value_loss = 48552.54296875, stopping_round = 12\n",
            "14. Current value loss = 1413951.625, min value_loss = 48552.54296875, stopping_round = 13\n",
            "15. Current value loss = 495346.96875, min value_loss = 48552.54296875, stopping_round = 14\n",
            "16. Current value loss = 648011.1875, min value_loss = 48552.54296875, stopping_round = 15\n",
            "17. Current value loss = 220353.25, min value_loss = 48552.54296875, stopping_round = 16\n",
            "18. Current value loss = 165791.296875, min value_loss = 48552.54296875, stopping_round = 17\n",
            "19. Current value loss = 265361.40625, min value_loss = 48552.54296875, stopping_round = 18\n",
            "20. Current value loss = 572598.1875, min value_loss = 48552.54296875, stopping_round = 19\n",
            "21. Current value loss = 284150.9375, min value_loss = 48552.54296875, stopping_round = 20\n",
            "22. Current value loss = 286593.625, min value_loss = 48552.54296875, stopping_round = 21\n",
            "23. Current value loss = 286151.90625, min value_loss = 48552.54296875, stopping_round = 22\n",
            "24. Current value loss = 495125.84375, min value_loss = 48552.54296875, stopping_round = 23\n",
            "25. Current value loss = 647365.4375, min value_loss = 48552.54296875, stopping_round = 24\n",
            "26. Current value loss = 803216.625, min value_loss = 48552.54296875, stopping_round = 25\n",
            "27. Current value loss = 1001246.875, min value_loss = 48552.54296875, stopping_round = 26\n",
            "28. Current value loss = 855699.0, min value_loss = 48552.54296875, stopping_round = 27\n",
            "29. Current value loss = 408063.34375, min value_loss = 48552.54296875, stopping_round = 28\n",
            "30. Current value loss = 1091916.0, min value_loss = 48552.54296875, stopping_round = 29\n",
            "31. Current value loss = 2295268.0, min value_loss = 48552.54296875, stopping_round = 30\n",
            "32. Current value loss = 2183807.75, min value_loss = 48552.54296875, stopping_round = 31\n",
            "33. Current value loss = 2411523.5, min value_loss = 48552.54296875, stopping_round = 32\n",
            "34. Current value loss = 3651368.0, min value_loss = 48552.54296875, stopping_round = 33\n",
            "35. Current value loss = 1515942.125, min value_loss = 48552.54296875, stopping_round = 34\n",
            "36. Current value loss = 1156122.875, min value_loss = 48552.54296875, stopping_round = 35\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-197-d6d5504ec2dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"env = gym.make('CartPole-v0')\\npolicy_net = PolicyNetwork(env.observation_space.shape[0], env.action_space.n, 256)\\nvalue_net = ValueNetwork(env.observation_space.shape[0], env.action_space.n, 256)\\nbaseline_approximation(policy_net, value_net, 15, early_stopping_rounds=150)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-191-d98710a69944>\u001b[0m in \u001b[0;36mbaseline_approximation\u001b[0;34m(policy_net, value_net, n_trajectories, n_epoch, early_stopping_rounds)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mbaseline_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malign_columns_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mtraj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_trajectory_for_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mr_gradient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-196-9989cbe6b6d9>\u001b[0m in \u001b[0;36mcount_trajectory_for_baseline\u001b[0;34m(rewards, log_probs, baseline)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlog_prob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlog_prob\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mlog_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mderiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cCpwMpFFN3P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6c9b3fd3-cfff-4b75-99ad-24a46278c85e"
      },
      "source": [
        "%%time\n",
        "statistics_all = []\n",
        "statistics_mean = []\n",
        "for game in tqdm(range(10)):\n",
        "  all_rewards, mean_rewards = cart_pole_baseline(value_net, 1, 1500)\n",
        "  statistics_all.append(all_rewards)\n",
        "  statistics_mean.append(mean_rewards)"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "episode: 10, total mean reward among trajectories: 16.0, average_reward_among_trajectories: 22.2\n",
            "episode: 20, total mean reward among trajectories: 31.0, average_reward_among_trajectories: 25.8\n",
            "episode: 30, total mean reward among trajectories: 25.0, average_reward_among_trajectories: 26.6\n",
            "episode: 40, total mean reward among trajectories: 68.0, average_reward_among_trajectories: 33.4\n",
            "episode: 50, total mean reward among trajectories: 45.0, average_reward_among_trajectories: 33.2\n",
            "episode: 60, total mean reward among trajectories: 30.0, average_reward_among_trajectories: 29.4\n",
            "episode: 70, total mean reward among trajectories: 28.0, average_reward_among_trajectories: 21.9\n",
            "episode: 80, total mean reward among trajectories: 44.0, average_reward_among_trajectories: 29.8\n",
            "episode: 90, total mean reward among trajectories: 35.0, average_reward_among_trajectories: 40.6\n",
            "episode: 100, total mean reward among trajectories: 16.0, average_reward_among_trajectories: 23.1\n",
            "episode: 110, total mean reward among trajectories: 47.0, average_reward_among_trajectories: 37.3\n",
            "episode: 120, total mean reward among trajectories: 27.0, average_reward_among_trajectories: 22.6\n",
            "episode: 130, total mean reward among trajectories: 77.0, average_reward_among_trajectories: 31.4\n",
            "episode: 140, total mean reward among trajectories: 28.0, average_reward_among_trajectories: 27.4\n",
            "episode: 150, total mean reward among trajectories: 20.0, average_reward_among_trajectories: 30.9\n",
            "episode: 160, total mean reward among trajectories: 21.0, average_reward_among_trajectories: 21.9\n",
            "episode: 170, total mean reward among trajectories: 21.0, average_reward_among_trajectories: 28.6\n",
            "episode: 180, total mean reward among trajectories: 22.0, average_reward_among_trajectories: 25.3\n",
            "episode: 190, total mean reward among trajectories: 23.0, average_reward_among_trajectories: 24.6\n",
            "episode: 200, total mean reward among trajectories: 22.0, average_reward_among_trajectories: 27.2\n",
            "episode: 210, total mean reward among trajectories: 44.0, average_reward_among_trajectories: 20.5\n",
            "episode: 220, total mean reward among trajectories: 31.0, average_reward_among_trajectories: 30.7\n",
            "episode: 230, total mean reward among trajectories: 27.0, average_reward_among_trajectories: 28.7\n",
            "episode: 240, total mean reward among trajectories: 11.0, average_reward_among_trajectories: 25.1\n",
            "episode: 250, total mean reward among trajectories: 22.0, average_reward_among_trajectories: 24.4\n",
            "episode: 260, total mean reward among trajectories: 79.0, average_reward_among_trajectories: 31.6\n",
            "episode: 270, total mean reward among trajectories: 35.0, average_reward_among_trajectories: 19.0\n",
            "episode: 280, total mean reward among trajectories: 40.0, average_reward_among_trajectories: 32.0\n",
            "episode: 290, total mean reward among trajectories: 17.0, average_reward_among_trajectories: 31.4\n",
            "episode: 300, total mean reward among trajectories: 15.0, average_reward_among_trajectories: 30.0\n",
            "episode: 310, total mean reward among trajectories: 19.0, average_reward_among_trajectories: 25.3\n",
            "episode: 320, total mean reward among trajectories: 32.0, average_reward_among_trajectories: 29.5\n",
            "episode: 330, total mean reward among trajectories: 28.0, average_reward_among_trajectories: 37.4\n",
            "episode: 340, total mean reward among trajectories: 18.0, average_reward_among_trajectories: 25.7\n",
            "episode: 350, total mean reward among trajectories: 14.0, average_reward_among_trajectories: 20.7\n",
            "episode: 360, total mean reward among trajectories: 35.0, average_reward_among_trajectories: 21.9\n",
            "episode: 370, total mean reward among trajectories: 13.0, average_reward_among_trajectories: 22.3\n",
            "episode: 380, total mean reward among trajectories: 25.0, average_reward_among_trajectories: 26.6\n",
            "episode: 390, total mean reward among trajectories: 36.0, average_reward_among_trajectories: 39.6\n",
            "episode: 400, total mean reward among trajectories: 26.0, average_reward_among_trajectories: 25.8\n",
            "episode: 410, total mean reward among trajectories: 15.0, average_reward_among_trajectories: 37.5\n",
            "episode: 420, total mean reward among trajectories: 47.0, average_reward_among_trajectories: 24.8\n",
            "episode: 430, total mean reward among trajectories: 16.0, average_reward_among_trajectories: 28.4\n",
            "episode: 440, total mean reward among trajectories: 15.0, average_reward_among_trajectories: 23.5\n",
            "episode: 450, total mean reward among trajectories: 29.0, average_reward_among_trajectories: 29.9\n",
            "episode: 460, total mean reward among trajectories: 21.0, average_reward_among_trajectories: 24.6\n",
            "episode: 470, total mean reward among trajectories: 46.0, average_reward_among_trajectories: 33.7\n",
            "episode: 480, total mean reward among trajectories: 92.0, average_reward_among_trajectories: 42.0\n",
            "episode: 490, total mean reward among trajectories: 22.0, average_reward_among_trajectories: 30.5\n",
            "episode: 500, total mean reward among trajectories: 33.0, average_reward_among_trajectories: 33.8\n",
            "episode: 510, total mean reward among trajectories: 23.0, average_reward_among_trajectories: 29.8\n",
            "episode: 520, total mean reward among trajectories: 42.0, average_reward_among_trajectories: 33.5\n",
            "episode: 530, total mean reward among trajectories: 48.0, average_reward_among_trajectories: 34.7\n",
            "episode: 540, total mean reward among trajectories: 65.0, average_reward_among_trajectories: 46.8\n",
            "episode: 550, total mean reward among trajectories: 54.0, average_reward_among_trajectories: 38.1\n",
            "episode: 560, total mean reward among trajectories: 20.0, average_reward_among_trajectories: 42.4\n",
            "episode: 570, total mean reward among trajectories: 68.0, average_reward_among_trajectories: 41.0\n",
            "episode: 580, total mean reward among trajectories: 16.0, average_reward_among_trajectories: 27.9\n",
            "episode: 590, total mean reward among trajectories: 34.0, average_reward_among_trajectories: 28.5\n",
            "episode: 600, total mean reward among trajectories: 10.0, average_reward_among_trajectories: 21.3\n",
            "episode: 610, total mean reward among trajectories: 28.0, average_reward_among_trajectories: 44.3\n",
            "episode: 620, total mean reward among trajectories: 42.0, average_reward_among_trajectories: 34.3\n",
            "episode: 630, total mean reward among trajectories: 17.0, average_reward_among_trajectories: 39.5\n",
            "episode: 640, total mean reward among trajectories: 23.0, average_reward_among_trajectories: 28.9\n",
            "episode: 650, total mean reward among trajectories: 25.0, average_reward_among_trajectories: 36.9\n",
            "episode: 660, total mean reward among trajectories: 15.0, average_reward_among_trajectories: 41.2\n",
            "episode: 670, total mean reward among trajectories: 12.0, average_reward_among_trajectories: 30.7\n",
            "episode: 680, total mean reward among trajectories: 68.0, average_reward_among_trajectories: 38.4\n",
            "episode: 690, total mean reward among trajectories: 50.0, average_reward_among_trajectories: 39.3\n",
            "episode: 700, total mean reward among trajectories: 28.0, average_reward_among_trajectories: 28.1\n",
            "episode: 710, total mean reward among trajectories: 75.0, average_reward_among_trajectories: 43.3\n",
            "episode: 720, total mean reward among trajectories: 13.0, average_reward_among_trajectories: 44.1\n",
            "episode: 730, total mean reward among trajectories: 76.0, average_reward_among_trajectories: 35.5\n",
            "episode: 740, total mean reward among trajectories: 59.0, average_reward_among_trajectories: 40.2\n",
            "episode: 750, total mean reward among trajectories: 39.0, average_reward_among_trajectories: 57.6\n",
            "episode: 760, total mean reward among trajectories: 38.0, average_reward_among_trajectories: 46.2\n",
            "episode: 770, total mean reward among trajectories: 27.0, average_reward_among_trajectories: 42.8\n",
            "episode: 780, total mean reward among trajectories: 22.0, average_reward_among_trajectories: 28.3\n",
            "episode: 790, total mean reward among trajectories: 67.0, average_reward_among_trajectories: 36.0\n",
            "episode: 800, total mean reward among trajectories: 19.0, average_reward_among_trajectories: 44.2\n",
            "episode: 810, total mean reward among trajectories: 39.0, average_reward_among_trajectories: 46.0\n",
            "episode: 820, total mean reward among trajectories: 38.0, average_reward_among_trajectories: 42.0\n",
            "episode: 830, total mean reward among trajectories: 56.0, average_reward_among_trajectories: 34.7\n",
            "episode: 840, total mean reward among trajectories: 48.0, average_reward_among_trajectories: 37.4\n",
            "episode: 850, total mean reward among trajectories: 15.0, average_reward_among_trajectories: 52.4\n",
            "episode: 860, total mean reward among trajectories: 49.0, average_reward_among_trajectories: 40.3\n",
            "episode: 870, total mean reward among trajectories: 20.0, average_reward_among_trajectories: 31.3\n",
            "episode: 880, total mean reward among trajectories: 18.0, average_reward_among_trajectories: 38.3\n",
            "episode: 890, total mean reward among trajectories: 26.0, average_reward_among_trajectories: 42.7\n",
            "episode: 900, total mean reward among trajectories: 79.0, average_reward_among_trajectories: 31.4\n",
            "episode: 910, total mean reward among trajectories: 69.0, average_reward_among_trajectories: 40.5\n",
            "episode: 920, total mean reward among trajectories: 63.0, average_reward_among_trajectories: 37.1\n",
            "episode: 930, total mean reward among trajectories: 29.0, average_reward_among_trajectories: 24.5\n",
            "episode: 940, total mean reward among trajectories: 23.0, average_reward_among_trajectories: 42.8\n",
            "episode: 950, total mean reward among trajectories: 48.0, average_reward_among_trajectories: 34.9\n",
            "episode: 960, total mean reward among trajectories: 62.0, average_reward_among_trajectories: 28.8\n",
            "episode: 970, total mean reward among trajectories: 30.0, average_reward_among_trajectories: 33.3\n",
            "episode: 980, total mean reward among trajectories: 17.0, average_reward_among_trajectories: 26.3\n",
            "episode: 990, total mean reward among trajectories: 32.0, average_reward_among_trajectories: 43.5\n",
            "episode: 1000, total mean reward among trajectories: 11.0, average_reward_among_trajectories: 35.8\n",
            "episode: 1010, total mean reward among trajectories: 16.0, average_reward_among_trajectories: 32.4\n",
            "episode: 1020, total mean reward among trajectories: 22.0, average_reward_among_trajectories: 33.1\n",
            "episode: 1030, total mean reward among trajectories: 40.0, average_reward_among_trajectories: 43.0\n",
            "episode: 1040, total mean reward among trajectories: 19.0, average_reward_among_trajectories: 41.3\n",
            "episode: 1050, total mean reward among trajectories: 28.0, average_reward_among_trajectories: 31.2\n",
            "episode: 1060, total mean reward among trajectories: 20.0, average_reward_among_trajectories: 36.4\n",
            "episode: 1070, total mean reward among trajectories: 18.0, average_reward_among_trajectories: 33.4\n",
            "episode: 1080, total mean reward among trajectories: 9.0, average_reward_among_trajectories: 26.4\n",
            "episode: 1090, total mean reward among trajectories: 24.0, average_reward_among_trajectories: 29.4\n",
            "episode: 1100, total mean reward among trajectories: 23.0, average_reward_among_trajectories: 36.0\n",
            "episode: 1110, total mean reward among trajectories: 17.0, average_reward_among_trajectories: 29.5\n",
            "episode: 1120, total mean reward among trajectories: 25.0, average_reward_among_trajectories: 29.9\n",
            "episode: 1130, total mean reward among trajectories: 28.0, average_reward_among_trajectories: 23.3\n",
            "episode: 1140, total mean reward among trajectories: 18.0, average_reward_among_trajectories: 22.3\n",
            "episode: 1150, total mean reward among trajectories: 39.0, average_reward_among_trajectories: 28.7\n",
            "episode: 1160, total mean reward among trajectories: 18.0, average_reward_among_trajectories: 27.0\n",
            "episode: 1170, total mean reward among trajectories: 101.0, average_reward_among_trajectories: 45.8\n",
            "episode: 1180, total mean reward among trajectories: 18.0, average_reward_among_trajectories: 32.3\n",
            "episode: 1190, total mean reward among trajectories: 32.0, average_reward_among_trajectories: 23.9\n",
            "episode: 1200, total mean reward among trajectories: 22.0, average_reward_among_trajectories: 23.7\n",
            "episode: 1210, total mean reward among trajectories: 44.0, average_reward_among_trajectories: 35.3\n",
            "episode: 1220, total mean reward among trajectories: 19.0, average_reward_among_trajectories: 29.5\n",
            "episode: 1230, total mean reward among trajectories: 14.0, average_reward_among_trajectories: 22.1\n",
            "episode: 1240, total mean reward among trajectories: 26.0, average_reward_among_trajectories: 25.9\n",
            "episode: 1250, total mean reward among trajectories: 41.0, average_reward_among_trajectories: 21.1\n",
            "episode: 1260, total mean reward among trajectories: 31.0, average_reward_among_trajectories: 24.3\n",
            "episode: 1270, total mean reward among trajectories: 20.0, average_reward_among_trajectories: 19.4\n",
            "episode: 1280, total mean reward among trajectories: 31.0, average_reward_among_trajectories: 22.3\n",
            "episode: 1290, total mean reward among trajectories: 17.0, average_reward_among_trajectories: 29.6\n",
            "episode: 1300, total mean reward among trajectories: 27.0, average_reward_among_trajectories: 18.4\n",
            "episode: 1310, total mean reward among trajectories: 34.0, average_reward_among_trajectories: 25.3\n",
            "episode: 1320, total mean reward among trajectories: 14.0, average_reward_among_trajectories: 20.1\n",
            "episode: 1330, total mean reward among trajectories: 59.0, average_reward_among_trajectories: 28.6\n",
            "episode: 1340, total mean reward among trajectories: 20.0, average_reward_among_trajectories: 23.9\n",
            "episode: 1350, total mean reward among trajectories: 43.0, average_reward_among_trajectories: 30.3\n",
            "episode: 1360, total mean reward among trajectories: 9.0, average_reward_among_trajectories: 26.8\n",
            "episode: 1370, total mean reward among trajectories: 27.0, average_reward_among_trajectories: 32.3\n",
            "episode: 1380, total mean reward among trajectories: 32.0, average_reward_among_trajectories: 21.1\n",
            "episode: 1390, total mean reward among trajectories: 29.0, average_reward_among_trajectories: 25.0\n",
            "episode: 1400, total mean reward among trajectories: 12.0, average_reward_among_trajectories: 25.9\n",
            "episode: 1410, total mean reward among trajectories: 20.0, average_reward_among_trajectories: 34.0\n",
            "episode: 1420, total mean reward among trajectories: 28.0, average_reward_among_trajectories: 25.4\n",
            "episode: 1430, total mean reward among trajectories: 37.0, average_reward_among_trajectories: 29.7\n",
            "episode: 1440, total mean reward among trajectories: 17.0, average_reward_among_trajectories: 26.3\n",
            "episode: 1450, total mean reward among trajectories: 35.0, average_reward_among_trajectories: 25.8\n",
            "episode: 1460, total mean reward among trajectories: 24.0, average_reward_among_trajectories: 21.5\n",
            "episode: 1470, total mean reward among trajectories: 25.0, average_reward_among_trajectories: 23.8\n",
            "episode: 1480, total mean reward among trajectories: 23.0, average_reward_among_trajectories: 24.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|â–ˆ         | 1/10 [00:34<05:10, 34.50s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "episode: 1490, total mean reward among trajectories: 12.0, average_reward_among_trajectories: 25.5\n",
            "episode: 1500, total mean reward among trajectories: 20.0, average_reward_among_trajectories: 22.4\n",
            "episode: 10, total mean reward among trajectories: 13.0, average_reward_among_trajectories: 28.4\n",
            "episode: 20, total mean reward among trajectories: 40.0, average_reward_among_trajectories: 21.6\n",
            "episode: 30, total mean reward among trajectories: 36.0, average_reward_among_trajectories: 23.2\n",
            "episode: 40, total mean reward among trajectories: 21.0, average_reward_among_trajectories: 19.1\n",
            "episode: 50, total mean reward among trajectories: 12.0, average_reward_among_trajectories: 16.6\n",
            "episode: 60, total mean reward among trajectories: 15.0, average_reward_among_trajectories: 19.0\n",
            "episode: 70, total mean reward among trajectories: 10.0, average_reward_among_trajectories: 21.1\n",
            "episode: 80, total mean reward among trajectories: 23.0, average_reward_among_trajectories: 19.5\n",
            "episode: 90, total mean reward among trajectories: 9.0, average_reward_among_trajectories: 20.7\n",
            "episode: 100, total mean reward among trajectories: 34.0, average_reward_among_trajectories: 26.4\n",
            "episode: 110, total mean reward among trajectories: 27.0, average_reward_among_trajectories: 24.3\n",
            "episode: 120, total mean reward among trajectories: 17.0, average_reward_among_trajectories: 28.8\n",
            "episode: 130, total mean reward among trajectories: 21.0, average_reward_among_trajectories: 17.3\n",
            "episode: 140, total mean reward among trajectories: 13.0, average_reward_among_trajectories: 15.7\n",
            "episode: 150, total mean reward among trajectories: 27.0, average_reward_among_trajectories: 19.8\n",
            "episode: 160, total mean reward among trajectories: 18.0, average_reward_among_trajectories: 18.9\n",
            "episode: 170, total mean reward among trajectories: 49.0, average_reward_among_trajectories: 26.2\n",
            "episode: 180, total mean reward among trajectories: 12.0, average_reward_among_trajectories: 18.3\n",
            "episode: 190, total mean reward among trajectories: 17.0, average_reward_among_trajectories: 19.8\n",
            "episode: 200, total mean reward among trajectories: 16.0, average_reward_among_trajectories: 18.3\n",
            "episode: 210, total mean reward among trajectories: 11.0, average_reward_among_trajectories: 19.4\n",
            "episode: 220, total mean reward among trajectories: 16.0, average_reward_among_trajectories: 17.3\n",
            "episode: 230, total mean reward among trajectories: 20.0, average_reward_among_trajectories: 26.0\n",
            "episode: 240, total mean reward among trajectories: 22.0, average_reward_among_trajectories: 23.7\n",
            "episode: 250, total mean reward among trajectories: 9.0, average_reward_among_trajectories: 20.7\n",
            "episode: 260, total mean reward among trajectories: 24.0, average_reward_among_trajectories: 20.4\n",
            "episode: 270, total mean reward among trajectories: 31.0, average_reward_among_trajectories: 18.9\n",
            "episode: 280, total mean reward among trajectories: 18.0, average_reward_among_trajectories: 19.1\n",
            "episode: 290, total mean reward among trajectories: 20.0, average_reward_among_trajectories: 16.6\n",
            "episode: 300, total mean reward among trajectories: 14.0, average_reward_among_trajectories: 19.2\n",
            "episode: 310, total mean reward among trajectories: 12.0, average_reward_among_trajectories: 22.2\n",
            "episode: 320, total mean reward among trajectories: 13.0, average_reward_among_trajectories: 21.8\n",
            "episode: 330, total mean reward among trajectories: 17.0, average_reward_among_trajectories: 18.5\n",
            "episode: 340, total mean reward among trajectories: 19.0, average_reward_among_trajectories: 17.2\n",
            "episode: 350, total mean reward among trajectories: 15.0, average_reward_among_trajectories: 15.4\n",
            "episode: 360, total mean reward among trajectories: 9.0, average_reward_among_trajectories: 17.4\n",
            "episode: 370, total mean reward among trajectories: 15.0, average_reward_among_trajectories: 18.6\n",
            "episode: 380, total mean reward among trajectories: 23.0, average_reward_among_trajectories: 19.1\n",
            "episode: 390, total mean reward among trajectories: 28.0, average_reward_among_trajectories: 17.5\n",
            "episode: 400, total mean reward among trajectories: 17.0, average_reward_among_trajectories: 20.6\n",
            "episode: 410, total mean reward among trajectories: 12.0, average_reward_among_trajectories: 22.0\n",
            "episode: 420, total mean reward among trajectories: 13.0, average_reward_among_trajectories: 21.2\n",
            "episode: 430, total mean reward among trajectories: 24.0, average_reward_among_trajectories: 24.3\n",
            "episode: 440, total mean reward among trajectories: 19.0, average_reward_among_trajectories: 22.2\n",
            "episode: 450, total mean reward among trajectories: 17.0, average_reward_among_trajectories: 21.6\n",
            "episode: 460, total mean reward among trajectories: 20.0, average_reward_among_trajectories: 21.6\n",
            "episode: 470, total mean reward among trajectories: 18.0, average_reward_among_trajectories: 18.8\n",
            "episode: 480, total mean reward among trajectories: 31.0, average_reward_among_trajectories: 25.1\n",
            "episode: 490, total mean reward among trajectories: 20.0, average_reward_among_trajectories: 20.3\n",
            "episode: 500, total mean reward among trajectories: 19.0, average_reward_among_trajectories: 19.2\n",
            "episode: 510, total mean reward among trajectories: 15.0, average_reward_among_trajectories: 21.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-194-8314cb01a94c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'statistics_all = []\\nstatistics_mean = []\\nfor game in tqdm(range(10)):\\n  all_rewards, mean_rewards = cart_pole_baseline(value_net, 1, 1500)\\n  statistics_all.append(all_rewards)\\n  statistics_mean.append(mean_rewards)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-73-b22b84ef37d7>\u001b[0m in \u001b[0;36mcart_pole_baseline\u001b[0;34m(value_net, n_trajectories, episode_num, baseline_retrain, retrain_episodes)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                     \u001b[0mtraj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_trajectory_for_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                     \u001b[0mr_gradient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mnumsteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-167-50f8bd785e90>\u001b[0m in \u001b[0;36mcount_trajectory_for_policy\u001b[0;34m(rewards, log_probs, baseline)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mpolicy_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscounted_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlog_prob\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mpolicy_gradient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m    460\u001b[0m                           \u001b[0;34m'iterations executed (and might lead to errors or silently give '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m                           'incorrect results).', category=RuntimeWarning)\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nw_4rhGv5sxd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "20e58bc3-6dcd-4ab8-8ae1-0fd09b82f713"
      },
      "source": [
        "plt.plot(np.mean(np.mean(statistics_all, axis = 1), axis = 0))\n",
        "plt.plot(np.mean(np.mean(statistics_mean, axis = 1), axis = 0))\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "print('Varince of reward = {}'.format(np.var(np.mean(np.mean(statistics_all, axis = 1), axis = 0))))"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGM//A1CzevrlwMS6AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Varince of reward = 3467.4896215743997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-1bLfv3mo5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}