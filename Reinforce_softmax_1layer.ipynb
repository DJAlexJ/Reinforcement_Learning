{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch  \n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "#from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def softmax_grad(s): \n",
    "    jacobian_m = np.diag(s)\n",
    "    for i in range(len(jacobian_m)):\n",
    "        for j in range(len(jacobian_m)):\n",
    "            if i == j:\n",
    "                jacobian_m[i][j] = s[i] * (1-s[i])\n",
    "            else: \n",
    "                jacobian_m[i][j] = -s[i]*s[j]\n",
    "    return jacobian_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(policy, rewards, log_probs):\n",
    "    discounted_rewards = []\n",
    "\n",
    "    for t in range(len(rewards)):\n",
    "        Gt = 0 \n",
    "        pw = 0\n",
    "        for r in rewards[t:]:\n",
    "            Gt = Gt + GAMMA**pw * r\n",
    "            pw = pw + 1\n",
    "        discounted_rewards.append(Gt)\n",
    "        \n",
    "    discounted_rewards = (discounted_rewards - np.mean(discounted_rewards)) / (np.std(discounted_rewards) + 1e-9) # normalize discounted rewards\n",
    "\n",
    "    policy_gradient = []\n",
    "    for log_prob, Gt in zip(log_probs, discounted_rewards):\n",
    "        policy_gradient.append(-log_prob * Gt)\n",
    "    \n",
    "    #print(policy_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, total reward: 13.0, average_reward: 13.0, length: 12\n",
      "episode: 10, total reward: 16.0, average_reward: 32.6, length: 15\n",
      "episode: 20, total reward: 16.0, average_reward: 22.8, length: 15\n",
      "episode: 30, total reward: 26.0, average_reward: 20.2, length: 25\n",
      "episode: 40, total reward: 13.0, average_reward: 23.7, length: 12\n",
      "episode: 50, total reward: 17.0, average_reward: 23.5, length: 16\n",
      "episode: 60, total reward: 27.0, average_reward: 27.8, length: 26\n",
      "episode: 70, total reward: 12.0, average_reward: 17.2, length: 11\n",
      "episode: 80, total reward: 15.0, average_reward: 21.0, length: 14\n",
      "episode: 90, total reward: 14.0, average_reward: 21.4, length: 13\n",
      "episode: 100, total reward: 18.0, average_reward: 18.0, length: 17\n",
      "episode: 110, total reward: 12.0, average_reward: 18.0, length: 11\n",
      "episode: 120, total reward: 48.0, average_reward: 28.2, length: 47\n",
      "episode: 130, total reward: 15.0, average_reward: 20.3, length: 14\n",
      "episode: 140, total reward: 13.0, average_reward: 24.6, length: 12\n",
      "episode: 150, total reward: 22.0, average_reward: 21.9, length: 21\n",
      "episode: 160, total reward: 22.0, average_reward: 25.5, length: 21\n",
      "episode: 170, total reward: 38.0, average_reward: 26.0, length: 37\n",
      "episode: 180, total reward: 13.0, average_reward: 16.1, length: 12\n",
      "episode: 190, total reward: 24.0, average_reward: 26.3, length: 23\n",
      "episode: 200, total reward: 12.0, average_reward: 25.4, length: 11\n",
      "episode: 210, total reward: 22.0, average_reward: 22.5, length: 21\n",
      "episode: 220, total reward: 24.0, average_reward: 18.1, length: 23\n",
      "episode: 230, total reward: 10.0, average_reward: 20.1, length: 9\n",
      "episode: 240, total reward: 16.0, average_reward: 24.4, length: 15\n",
      "episode: 250, total reward: 39.0, average_reward: 22.9, length: 38\n",
      "episode: 260, total reward: 11.0, average_reward: 22.0, length: 10\n",
      "episode: 270, total reward: 10.0, average_reward: 24.0, length: 9\n",
      "episode: 280, total reward: 13.0, average_reward: 17.0, length: 12\n",
      "episode: 290, total reward: 15.0, average_reward: 20.9, length: 14\n",
      "episode: 300, total reward: 42.0, average_reward: 25.3, length: 41\n",
      "episode: 310, total reward: 23.0, average_reward: 27.4, length: 22\n",
      "episode: 320, total reward: 16.0, average_reward: 29.1, length: 15\n",
      "episode: 330, total reward: 21.0, average_reward: 27.5, length: 20\n",
      "episode: 340, total reward: 26.0, average_reward: 23.6, length: 25\n",
      "episode: 350, total reward: 29.0, average_reward: 17.2, length: 28\n",
      "episode: 360, total reward: 25.0, average_reward: 23.0, length: 24\n",
      "episode: 370, total reward: 20.0, average_reward: 22.1, length: 19\n",
      "episode: 380, total reward: 14.0, average_reward: 26.7, length: 13\n",
      "episode: 390, total reward: 41.0, average_reward: 25.0, length: 40\n",
      "episode: 400, total reward: 13.0, average_reward: 24.4, length: 12\n",
      "episode: 410, total reward: 11.0, average_reward: 20.0, length: 10\n",
      "episode: 420, total reward: 34.0, average_reward: 23.9, length: 33\n",
      "episode: 430, total reward: 12.0, average_reward: 23.6, length: 11\n",
      "episode: 440, total reward: 34.0, average_reward: 26.7, length: 33\n",
      "episode: 450, total reward: 23.0, average_reward: 21.2, length: 22\n",
      "episode: 460, total reward: 13.0, average_reward: 21.7, length: 12\n",
      "episode: 470, total reward: 35.0, average_reward: 32.7, length: 34\n",
      "episode: 480, total reward: 31.0, average_reward: 22.9, length: 30\n",
      "episode: 490, total reward: 67.0, average_reward: 27.8, length: 66\n",
      "episode: 500, total reward: 36.0, average_reward: 20.8, length: 35\n",
      "episode: 510, total reward: 25.0, average_reward: 22.8, length: 24\n",
      "episode: 520, total reward: 53.0, average_reward: 23.0, length: 52\n",
      "episode: 530, total reward: 15.0, average_reward: 25.0, length: 14\n",
      "episode: 540, total reward: 14.0, average_reward: 24.2, length: 13\n",
      "episode: 550, total reward: 25.0, average_reward: 21.2, length: 24\n",
      "episode: 560, total reward: 25.0, average_reward: 20.8, length: 24\n",
      "episode: 570, total reward: 15.0, average_reward: 21.2, length: 14\n",
      "episode: 580, total reward: 28.0, average_reward: 25.7, length: 27\n",
      "episode: 590, total reward: 19.0, average_reward: 19.6, length: 18\n",
      "episode: 600, total reward: 30.0, average_reward: 27.9, length: 29\n",
      "episode: 610, total reward: 15.0, average_reward: 20.1, length: 14\n",
      "episode: 620, total reward: 16.0, average_reward: 28.8, length: 15\n",
      "episode: 630, total reward: 19.0, average_reward: 26.5, length: 18\n",
      "episode: 640, total reward: 45.0, average_reward: 27.5, length: 44\n",
      "episode: 650, total reward: 23.0, average_reward: 28.0, length: 22\n",
      "episode: 660, total reward: 14.0, average_reward: 22.8, length: 13\n",
      "episode: 670, total reward: 19.0, average_reward: 32.8, length: 18\n",
      "episode: 680, total reward: 18.0, average_reward: 20.4, length: 17\n",
      "episode: 690, total reward: 29.0, average_reward: 31.5, length: 28\n",
      "episode: 700, total reward: 30.0, average_reward: 22.9, length: 29\n",
      "episode: 710, total reward: 15.0, average_reward: 20.4, length: 14\n",
      "episode: 720, total reward: 11.0, average_reward: 19.4, length: 10\n",
      "episode: 730, total reward: 34.0, average_reward: 30.8, length: 33\n",
      "episode: 740, total reward: 24.0, average_reward: 22.8, length: 23\n",
      "episode: 750, total reward: 21.0, average_reward: 25.6, length: 20\n",
      "episode: 760, total reward: 26.0, average_reward: 20.7, length: 25\n",
      "episode: 770, total reward: 9.0, average_reward: 20.6, length: 8\n",
      "episode: 780, total reward: 24.0, average_reward: 23.3, length: 23\n",
      "episode: 790, total reward: 10.0, average_reward: 19.3, length: 9\n",
      "episode: 800, total reward: 23.0, average_reward: 18.6, length: 22\n",
      "episode: 810, total reward: 15.0, average_reward: 30.1, length: 14\n",
      "episode: 820, total reward: 20.0, average_reward: 27.9, length: 19\n",
      "episode: 830, total reward: 19.0, average_reward: 24.4, length: 18\n",
      "episode: 840, total reward: 62.0, average_reward: 24.7, length: 61\n",
      "episode: 850, total reward: 16.0, average_reward: 23.3, length: 15\n",
      "episode: 860, total reward: 21.0, average_reward: 21.6, length: 20\n",
      "episode: 870, total reward: 14.0, average_reward: 37.2, length: 13\n",
      "episode: 880, total reward: 19.0, average_reward: 19.5, length: 18\n",
      "episode: 890, total reward: 58.0, average_reward: 27.8, length: 57\n",
      "episode: 900, total reward: 13.0, average_reward: 15.8, length: 12\n",
      "episode: 910, total reward: 17.0, average_reward: 22.8, length: 16\n",
      "episode: 920, total reward: 30.0, average_reward: 17.6, length: 29\n",
      "episode: 930, total reward: 15.0, average_reward: 19.1, length: 14\n",
      "episode: 940, total reward: 30.0, average_reward: 34.0, length: 29\n",
      "episode: 950, total reward: 31.0, average_reward: 21.9, length: 30\n",
      "episode: 960, total reward: 18.0, average_reward: 26.6, length: 17\n",
      "episode: 970, total reward: 32.0, average_reward: 22.9, length: 31\n",
      "episode: 980, total reward: 15.0, average_reward: 21.7, length: 14\n",
      "episode: 990, total reward: 17.0, average_reward: 19.1, length: 16\n",
      "episode: 1000, total reward: 17.0, average_reward: 26.1, length: 16\n",
      "episode: 1010, total reward: 27.0, average_reward: 21.1, length: 26\n",
      "episode: 1020, total reward: 29.0, average_reward: 24.2, length: 28\n",
      "episode: 1030, total reward: 36.0, average_reward: 33.3, length: 35\n",
      "episode: 1040, total reward: 35.0, average_reward: 20.3, length: 34\n",
      "episode: 1050, total reward: 45.0, average_reward: 23.0, length: 44\n",
      "episode: 1060, total reward: 11.0, average_reward: 19.7, length: 10\n",
      "episode: 1070, total reward: 42.0, average_reward: 22.2, length: 41\n",
      "episode: 1080, total reward: 20.0, average_reward: 20.1, length: 19\n",
      "episode: 1090, total reward: 22.0, average_reward: 23.2, length: 21\n",
      "episode: 1100, total reward: 11.0, average_reward: 24.4, length: 10\n",
      "episode: 1110, total reward: 15.0, average_reward: 22.5, length: 14\n",
      "episode: 1120, total reward: 26.0, average_reward: 18.2, length: 25\n",
      "episode: 1130, total reward: 20.0, average_reward: 22.3, length: 19\n",
      "episode: 1140, total reward: 17.0, average_reward: 27.5, length: 16\n",
      "episode: 1150, total reward: 18.0, average_reward: 30.4, length: 17\n",
      "episode: 1160, total reward: 45.0, average_reward: 20.7, length: 44\n",
      "episode: 1170, total reward: 11.0, average_reward: 20.4, length: 10\n",
      "episode: 1180, total reward: 15.0, average_reward: 26.8, length: 14\n",
      "episode: 1190, total reward: 13.0, average_reward: 26.5, length: 12\n",
      "episode: 1200, total reward: 10.0, average_reward: 32.5, length: 9\n",
      "episode: 1210, total reward: 15.0, average_reward: 19.4, length: 14\n",
      "episode: 1220, total reward: 36.0, average_reward: 28.2, length: 35\n",
      "episode: 1230, total reward: 16.0, average_reward: 27.6, length: 15\n",
      "episode: 1240, total reward: 12.0, average_reward: 24.1, length: 11\n",
      "episode: 1250, total reward: 11.0, average_reward: 21.4, length: 10\n",
      "episode: 1260, total reward: 11.0, average_reward: 19.1, length: 10\n",
      "episode: 1270, total reward: 21.0, average_reward: 26.1, length: 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1280, total reward: 9.0, average_reward: 23.7, length: 8\n",
      "episode: 1290, total reward: 15.0, average_reward: 18.7, length: 14\n",
      "episode: 1300, total reward: 11.0, average_reward: 21.7, length: 10\n",
      "episode: 1310, total reward: 31.0, average_reward: 22.9, length: 30\n",
      "episode: 1320, total reward: 42.0, average_reward: 25.1, length: 41\n",
      "episode: 1330, total reward: 26.0, average_reward: 19.1, length: 25\n",
      "episode: 1340, total reward: 42.0, average_reward: 24.3, length: 41\n",
      "episode: 1350, total reward: 13.0, average_reward: 17.6, length: 12\n",
      "episode: 1360, total reward: 25.0, average_reward: 23.5, length: 24\n",
      "episode: 1370, total reward: 14.0, average_reward: 20.0, length: 13\n",
      "episode: 1380, total reward: 8.0, average_reward: 26.9, length: 7\n",
      "episode: 1390, total reward: 18.0, average_reward: 20.6, length: 17\n",
      "episode: 1400, total reward: 26.0, average_reward: 28.2, length: 25\n",
      "episode: 1410, total reward: 12.0, average_reward: 23.5, length: 11\n",
      "episode: 1420, total reward: 19.0, average_reward: 23.8, length: 18\n",
      "episode: 1430, total reward: 23.0, average_reward: 25.8, length: 22\n",
      "episode: 1440, total reward: 18.0, average_reward: 15.9, length: 17\n",
      "episode: 1450, total reward: 15.0, average_reward: 22.9, length: 14\n",
      "episode: 1460, total reward: 24.0, average_reward: 28.9, length: 23\n",
      "episode: 1470, total reward: 119.0, average_reward: 32.9, length: 118\n",
      "episode: 1480, total reward: 13.0, average_reward: 42.1, length: 12\n",
      "episode: 1490, total reward: 17.0, average_reward: 26.6, length: 16\n",
      "episode: 1500, total reward: 22.0, average_reward: 19.9, length: 21\n",
      "episode: 1510, total reward: 11.0, average_reward: 25.2, length: 10\n",
      "episode: 1520, total reward: 15.0, average_reward: 16.6, length: 14\n",
      "episode: 1530, total reward: 15.0, average_reward: 22.6, length: 14\n",
      "episode: 1540, total reward: 86.0, average_reward: 30.9, length: 85\n",
      "episode: 1550, total reward: 26.0, average_reward: 32.4, length: 25\n",
      "episode: 1560, total reward: 21.0, average_reward: 24.1, length: 20\n",
      "episode: 1570, total reward: 14.0, average_reward: 21.0, length: 13\n",
      "episode: 1580, total reward: 14.0, average_reward: 28.1, length: 13\n",
      "episode: 1590, total reward: 44.0, average_reward: 24.2, length: 43\n",
      "episode: 1600, total reward: 11.0, average_reward: 20.5, length: 10\n",
      "episode: 1610, total reward: 15.0, average_reward: 21.9, length: 14\n",
      "episode: 1620, total reward: 11.0, average_reward: 17.3, length: 10\n",
      "episode: 1630, total reward: 38.0, average_reward: 32.1, length: 37\n",
      "episode: 1640, total reward: 15.0, average_reward: 16.4, length: 14\n",
      "episode: 1650, total reward: 15.0, average_reward: 19.0, length: 14\n",
      "episode: 1660, total reward: 26.0, average_reward: 25.2, length: 25\n",
      "episode: 1670, total reward: 19.0, average_reward: 24.1, length: 18\n",
      "episode: 1680, total reward: 20.0, average_reward: 29.0, length: 19\n",
      "episode: 1690, total reward: 10.0, average_reward: 25.3, length: 9\n",
      "episode: 1700, total reward: 20.0, average_reward: 19.2, length: 19\n",
      "episode: 1710, total reward: 19.0, average_reward: 31.6, length: 18\n",
      "episode: 1720, total reward: 72.0, average_reward: 28.7, length: 71\n",
      "episode: 1730, total reward: 16.0, average_reward: 18.3, length: 15\n",
      "episode: 1740, total reward: 10.0, average_reward: 30.7, length: 9\n",
      "episode: 1750, total reward: 32.0, average_reward: 28.4, length: 31\n",
      "episode: 1760, total reward: 19.0, average_reward: 20.0, length: 18\n",
      "episode: 1770, total reward: 12.0, average_reward: 34.7, length: 11\n",
      "episode: 1780, total reward: 10.0, average_reward: 13.7, length: 9\n",
      "episode: 1790, total reward: 21.0, average_reward: 22.0, length: 20\n",
      "episode: 1800, total reward: 17.0, average_reward: 22.8, length: 16\n",
      "episode: 1810, total reward: 15.0, average_reward: 26.0, length: 14\n",
      "episode: 1820, total reward: 19.0, average_reward: 24.8, length: 18\n",
      "episode: 1830, total reward: 22.0, average_reward: 27.8, length: 21\n",
      "episode: 1840, total reward: 24.0, average_reward: 26.2, length: 23\n",
      "episode: 1850, total reward: 13.0, average_reward: 27.4, length: 12\n",
      "episode: 1860, total reward: 11.0, average_reward: 23.8, length: 10\n",
      "episode: 1870, total reward: 22.0, average_reward: 30.5, length: 21\n",
      "episode: 1880, total reward: 27.0, average_reward: 19.1, length: 26\n",
      "episode: 1890, total reward: 35.0, average_reward: 25.4, length: 34\n",
      "episode: 1900, total reward: 44.0, average_reward: 30.7, length: 43\n",
      "episode: 1910, total reward: 23.0, average_reward: 24.8, length: 22\n",
      "episode: 1920, total reward: 15.0, average_reward: 21.7, length: 14\n",
      "episode: 1930, total reward: 15.0, average_reward: 24.0, length: 14\n",
      "episode: 1940, total reward: 33.0, average_reward: 25.4, length: 32\n",
      "episode: 1950, total reward: 13.0, average_reward: 25.8, length: 12\n",
      "episode: 1960, total reward: 31.0, average_reward: 20.3, length: 30\n",
      "episode: 1970, total reward: 11.0, average_reward: 19.6, length: 10\n",
      "episode: 1980, total reward: 19.0, average_reward: 25.6, length: 18\n",
      "episode: 1990, total reward: 18.0, average_reward: 29.7, length: 17\n",
      "episode: 2000, total reward: 41.0, average_reward: 20.7, length: 40\n",
      "episode: 2010, total reward: 36.0, average_reward: 21.9, length: 35\n",
      "episode: 2020, total reward: 18.0, average_reward: 32.1, length: 17\n",
      "episode: 2030, total reward: 11.0, average_reward: 27.9, length: 10\n",
      "episode: 2040, total reward: 14.0, average_reward: 23.5, length: 13\n",
      "episode: 2050, total reward: 20.0, average_reward: 21.7, length: 19\n",
      "episode: 2060, total reward: 22.0, average_reward: 16.9, length: 21\n",
      "episode: 2070, total reward: 20.0, average_reward: 26.8, length: 19\n",
      "episode: 2080, total reward: 16.0, average_reward: 32.8, length: 15\n",
      "episode: 2090, total reward: 42.0, average_reward: 21.8, length: 41\n",
      "episode: 2100, total reward: 14.0, average_reward: 23.6, length: 13\n",
      "episode: 2110, total reward: 30.0, average_reward: 19.0, length: 29\n",
      "episode: 2120, total reward: 31.0, average_reward: 17.4, length: 30\n",
      "episode: 2130, total reward: 14.0, average_reward: 18.5, length: 13\n",
      "episode: 2140, total reward: 43.0, average_reward: 26.5, length: 42\n",
      "episode: 2150, total reward: 12.0, average_reward: 25.5, length: 11\n",
      "episode: 2160, total reward: 39.0, average_reward: 24.6, length: 38\n",
      "episode: 2170, total reward: 23.0, average_reward: 30.1, length: 22\n",
      "episode: 2180, total reward: 22.0, average_reward: 23.5, length: 21\n",
      "episode: 2190, total reward: 13.0, average_reward: 23.4, length: 12\n",
      "episode: 2200, total reward: 20.0, average_reward: 21.8, length: 19\n",
      "episode: 2210, total reward: 27.0, average_reward: 28.5, length: 26\n",
      "episode: 2220, total reward: 11.0, average_reward: 24.6, length: 10\n",
      "episode: 2230, total reward: 11.0, average_reward: 16.2, length: 10\n",
      "episode: 2240, total reward: 23.0, average_reward: 20.0, length: 22\n",
      "episode: 2250, total reward: 11.0, average_reward: 18.0, length: 10\n",
      "episode: 2260, total reward: 18.0, average_reward: 20.7, length: 17\n",
      "episode: 2270, total reward: 43.0, average_reward: 24.2, length: 42\n",
      "episode: 2280, total reward: 30.0, average_reward: 25.0, length: 29\n",
      "episode: 2290, total reward: 21.0, average_reward: 25.0, length: 20\n",
      "episode: 2300, total reward: 33.0, average_reward: 29.1, length: 32\n",
      "episode: 2310, total reward: 17.0, average_reward: 25.6, length: 16\n",
      "episode: 2320, total reward: 13.0, average_reward: 19.7, length: 12\n",
      "episode: 2330, total reward: 26.0, average_reward: 32.9, length: 25\n",
      "episode: 2340, total reward: 24.0, average_reward: 26.3, length: 23\n",
      "episode: 2350, total reward: 19.0, average_reward: 28.9, length: 18\n",
      "episode: 2360, total reward: 11.0, average_reward: 23.2, length: 10\n",
      "episode: 2370, total reward: 20.0, average_reward: 21.6, length: 19\n",
      "episode: 2380, total reward: 23.0, average_reward: 23.9, length: 22\n",
      "episode: 2390, total reward: 47.0, average_reward: 25.3, length: 46\n",
      "episode: 2400, total reward: 13.0, average_reward: 17.8, length: 12\n",
      "episode: 2410, total reward: 28.0, average_reward: 21.5, length: 27\n",
      "episode: 2420, total reward: 23.0, average_reward: 26.7, length: 22\n",
      "episode: 2430, total reward: 10.0, average_reward: 25.1, length: 9\n",
      "episode: 2440, total reward: 19.0, average_reward: 21.2, length: 18\n",
      "episode: 2450, total reward: 38.0, average_reward: 25.3, length: 37\n",
      "episode: 2460, total reward: 15.0, average_reward: 23.8, length: 14\n",
      "episode: 2470, total reward: 46.0, average_reward: 22.7, length: 45\n",
      "episode: 2480, total reward: 33.0, average_reward: 22.4, length: 32\n",
      "episode: 2490, total reward: 26.0, average_reward: 23.4, length: 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2500, total reward: 9.0, average_reward: 19.0, length: 8\n",
      "episode: 2510, total reward: 15.0, average_reward: 29.0, length: 14\n",
      "episode: 2520, total reward: 23.0, average_reward: 24.9, length: 22\n",
      "episode: 2530, total reward: 11.0, average_reward: 17.0, length: 10\n",
      "episode: 2540, total reward: 19.0, average_reward: 22.7, length: 18\n",
      "episode: 2550, total reward: 15.0, average_reward: 17.7, length: 14\n",
      "episode: 2560, total reward: 14.0, average_reward: 24.0, length: 13\n",
      "episode: 2570, total reward: 15.0, average_reward: 23.2, length: 14\n",
      "episode: 2580, total reward: 33.0, average_reward: 19.5, length: 32\n",
      "episode: 2590, total reward: 10.0, average_reward: 21.3, length: 9\n",
      "episode: 2600, total reward: 33.0, average_reward: 21.4, length: 32\n",
      "episode: 2610, total reward: 50.0, average_reward: 21.7, length: 49\n",
      "episode: 2620, total reward: 19.0, average_reward: 23.3, length: 18\n",
      "episode: 2630, total reward: 10.0, average_reward: 27.3, length: 9\n",
      "episode: 2640, total reward: 61.0, average_reward: 25.2, length: 60\n",
      "episode: 2650, total reward: 13.0, average_reward: 22.2, length: 12\n",
      "episode: 2660, total reward: 18.0, average_reward: 20.1, length: 17\n",
      "episode: 2670, total reward: 21.0, average_reward: 17.0, length: 20\n",
      "episode: 2680, total reward: 13.0, average_reward: 25.7, length: 12\n",
      "episode: 2690, total reward: 12.0, average_reward: 27.5, length: 11\n",
      "episode: 2700, total reward: 87.0, average_reward: 31.6, length: 86\n",
      "episode: 2710, total reward: 25.0, average_reward: 22.8, length: 24\n",
      "episode: 2720, total reward: 15.0, average_reward: 23.8, length: 14\n",
      "episode: 2730, total reward: 30.0, average_reward: 16.8, length: 29\n",
      "episode: 2740, total reward: 30.0, average_reward: 22.8, length: 29\n",
      "episode: 2750, total reward: 11.0, average_reward: 21.0, length: 10\n",
      "episode: 2760, total reward: 12.0, average_reward: 19.9, length: 11\n",
      "episode: 2770, total reward: 26.0, average_reward: 22.7, length: 25\n",
      "episode: 2780, total reward: 34.0, average_reward: 20.6, length: 33\n",
      "episode: 2790, total reward: 19.0, average_reward: 20.5, length: 18\n",
      "episode: 2800, total reward: 15.0, average_reward: 25.1, length: 14\n",
      "episode: 2810, total reward: 22.0, average_reward: 26.4, length: 21\n",
      "episode: 2820, total reward: 26.0, average_reward: 21.6, length: 25\n",
      "episode: 2830, total reward: 14.0, average_reward: 20.7, length: 13\n",
      "episode: 2840, total reward: 21.0, average_reward: 23.8, length: 20\n",
      "episode: 2850, total reward: 19.0, average_reward: 27.4, length: 18\n",
      "episode: 2860, total reward: 18.0, average_reward: 24.1, length: 17\n",
      "episode: 2870, total reward: 16.0, average_reward: 22.1, length: 15\n",
      "episode: 2880, total reward: 143.0, average_reward: 36.3, length: 142\n",
      "episode: 2890, total reward: 34.0, average_reward: 22.8, length: 33\n",
      "episode: 2900, total reward: 30.0, average_reward: 19.0, length: 29\n",
      "episode: 2910, total reward: 38.0, average_reward: 31.6, length: 37\n",
      "episode: 2920, total reward: 37.0, average_reward: 31.2, length: 36\n",
      "episode: 2930, total reward: 32.0, average_reward: 23.3, length: 31\n",
      "episode: 2940, total reward: 36.0, average_reward: 18.0, length: 35\n",
      "episode: 2950, total reward: 15.0, average_reward: 24.6, length: 14\n",
      "episode: 2960, total reward: 16.0, average_reward: 18.3, length: 15\n"
     ]
    }
   ],
   "source": [
    "GAMMA = 0.99\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "num_actions = env.action_space.n\n",
    "max_episode_num = 3000\n",
    "max_steps = 10000\n",
    "numsteps = []\n",
    "avg_numsteps = []\n",
    "all_rewards = []\n",
    "policy = np.random.random_sample((env.observation_space.shape[0], env.action_space.n))\n",
    "\n",
    "for episode in range(max_episode_num):\n",
    "    state = env.reset().reshape(-1, 1).T\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "\n",
    "    for steps in range(max_steps):\n",
    "        #env.render()\n",
    "        probs = softmax(np.dot(state, policy))\n",
    "        action = np.random.choice(num_actions, p = np.squeeze(probs))\n",
    "        log_prob = np.log(np.squeeze(probs)[action])\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        if done:\n",
    "            update_policy(policy, rewards, log_probs)\n",
    "            numsteps.append(steps)\n",
    "            avg_numsteps.append(np.mean(numsteps[-10:]))\n",
    "            all_rewards.append(np.sum(rewards))\n",
    "            if episode % 10 == 0:\n",
    "                sys.stdout.write(\"episode: {}, total reward: {}, average_reward: {}, length: {}\\n\".format(episode, np.round(np.sum(rewards), decimals = 3),  np.round(np.mean(all_rewards[-10:]), decimals = 3), steps))\n",
    "            break\n",
    "            \n",
    "        state = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44759897, 0.96131853, 0.65431246, 0.74558678],\n",
       "       [0.90286341, 0.76139292, 0.23279973, 0.69732   ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
